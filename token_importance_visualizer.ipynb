{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dicts_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodebase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bert_predict_class, softmax, softmax_and_log_dicts\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot\n",
      "File \u001b[1;32mc:\\Users\\nilu8\\OneDrive\\Desktop\\Semantik\\MAYN_FEM_SEM\\codebase\\softmax_and_log_dicts.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdicts_count\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoftmax\u001b[39;00m\n\u001b[0;32m      5\u001b[0m LOG_DICTS \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dicts_count'"
     ]
    }
   ],
   "source": [
    "from codebase import bert_predict_class, softmax, softmax_and_log_dicts\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosinus Ähnlichkeit\n",
    "def get_cosine_sim(A: list[float], B: list[float]) -> float:\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "# Nimmt einen Satz und gibt einen Array mit Sätzen. Jeder von ihnen hat ein [UNK] Token von links nach rechts.\n",
    "# [MASK] wird nicht zu [UNK] gemapped.\n",
    "def get_sub_sentences(sentence: str) -> list[str]:\n",
    "    sub_sentences = list()\n",
    "    tokens = sentence[:-1].split(' ')\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        s = \"\"\n",
    "        for j in range(len(tokens)):\n",
    "            if i == j:\n",
    "                if tokens[j] == '[MASK]':\n",
    "                    s += '[MASK] '\n",
    "                else:\n",
    "                    s += '[UNK] '\n",
    "            else:\n",
    "                s += tokens[j] + ' '\n",
    "\n",
    "        sub_sentences.append(s[:-1] + '.')\n",
    "\n",
    "    return sub_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit dicts vergleichen\n",
    "def predict_class(bert_predictions, class_dicts=softmax_and_log_dicts.LOG_DICTS):\n",
    "    scores = []\n",
    "    score = 0\n",
    "    for d in class_dicts:\n",
    "        for prediction in bert_predictions:\n",
    "            try:\n",
    "                verb = bert_predict_class.get_verb_lemma(prediction['token_str'])\n",
    "                score += d[0][verb] * prediction['score']\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        scores.append((score))\n",
    "        score = 0\n",
    "    \n",
    "    # softmax und sortiere Score und Labels\n",
    "    softmaxed_scores = softmax.softmax(np.array(scores))\n",
    "    return softmaxed_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: markiere die Wörter, die mit XML-Tags markiert waren im Output-Satz\n",
    "orig_sentence = \"The <e1>bacterial aerosol</e1> was generated from an up-draft <e2>nebulizer</e2>.\"\n",
    "\n",
    "xml_indices = []\n",
    "for index, token in enumerate(orig_sentence.split(' ')):\n",
    "    if re.search('<e[12]>.*</e[12]>', token):\n",
    "        xml_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The bacterial aerosol was [MASK] from an up-draft nebulizer.\"\n",
    "# Vorhersage für den originalen Satz\n",
    "bert_pred = bert_predict_class.bert_predict(sentence)\n",
    "class_pred = predict_class(bert_pred)\n",
    "\n",
    "class_predictions = []\n",
    "for sub_sentence in get_sub_sentences(sentence):\n",
    "    bert_sub_pred = bert_predict_class.bert_predict(sub_sentence)\n",
    "    class_sub_pred = predict_class(bert_sub_pred)\n",
    "    class_predictions.append(class_sub_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "for p in class_predictions:\n",
    "    cosine_similarities.append(get_cosine_sim(p, class_pred))\n",
    "\n",
    "\n",
    "# Rechne min und max\n",
    "X_min = min(cosine_similarities)\n",
    "X_max = max(cosine_similarities)\n",
    "\n",
    "# Scaliere den Array\n",
    "scaled_similarities = [(x - X_min) / (X_max - X_min) for x in cosine_similarities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.765730937621949,\n",
       " 0.8018710934232024,\n",
       " 0.9815879621232879,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.24914962826139617,\n",
       " 0.7689371864670127,\n",
       " 0.9066516595595822,\n",
       " 0.7990113340250529]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-direction: row; font-size: xx-large; '><div style='color: rgb(255, 195.261389093597, 195.261389093597)'>The&nbsp</div><div style='color: rgb(255, 204.4771288229166, 204.4771288229166)'>bacterial&nbsp</div><div style='color: rgb(255, 250.30493034143842, 250.30493034143842)'>aerosol&nbsp</div><div style='color: rgb(255, 0.0, 0.0)'>was&nbsp</div><div style='color: rgb(255, 255.0, 255.0)'>[MASK]&nbsp</div><div style='color: rgb(255, 63.53315520665602, 63.53315520665602)'>from&nbsp</div><div style='color: rgb(255, 196.07898254908824, 196.07898254908824)'>an&nbsp</div><div style='color: rgb(255, 231.19617318769346, 231.19617318769346)'>up-draft&nbsp</div><div style='color: rgb(255, 203.7478901763885, 203.7478901763885)'>nebulizer</div>.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "STR = \"<div style='display: flex; flex-direction: row; font-size: xx-large; '>\"\n",
    "for index, token in enumerate(sentence[:-1].split(' ')):\n",
    "    STR += f\"<div style='color: rgb(255, {scaled_similarities[index] * 255}, {scaled_similarities[index] * 255})'>{token}&nbsp</div>\"\n",
    "STR = STR[:-11] + \"</div>.</div>\"\n",
    "\n",
    "HTML(STR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

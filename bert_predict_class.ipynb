{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IA = {'use': 76, 'apply': 16, 'take': 13, 'be': 10, 'attach': 9, 'create': 9, 'wield': 9, 'kill': 8, 'power': 6, 'play': 5, 'clean': 5, 'build': 5, 'form': 5, 'manipulate': 5, 'drive': 5, 'carry': 4, 'work': 4, 'fix': 4, 'mark': 4, 'give': 4, 'turn': 4, 'adjust': 4, 'show': 4, 'perform': 4, 'scale': 4, 'find': 3, 'stir': 3, 'choose': 3, 'acces': 3, 'determine': 3, 'treat': 3, 'tear': 3, 'fly': 3, 'threaten': 3, 'stop': 3, 'strike': 3, 'proces': 3, 'have': 3, 'start': 3, 'br': 3, 'move': 2, 'implement': 2, 'break': 2, 'calibrate': 2, 'approach': 2, 'resolve': 2, 'assist': 2, 'write': 2, 'cover': 2, 'identify': 2, 'study': 2, 'make': 2, 'touch': 2, 'help': 2, 'bake': 2, 'offer': 2, 'improve': 2, 'carve': 2, 'employ': 2, 'receive': 2, 'buy': 2, 'come': 2, 'win': 2, 'relax': 2, 'wear': 2, 'eat': 2, 'push': 2, 'put': 2, 'examine': 2, 'need': 2, 'remove': 2, 'deface': 2, 'obtain': 2, 'batter': 2, 'observe': 2, 'hold': 2, 'sweep': 2, 'operate': 2, 'travel': 2, 'get': 2, 'saw': 2, 'establish': 2, 'swap': 1, 'propose': 1, 'sew': 1, 'peel': 1, 'benefit': 1, 'plow': 1, 'catch': 1, 'drink': 1, 'filter': 1, 'locate': 1, 'asses': 1, 'mix': 1, 'date': 1, 'coat': 1, 'regain': 1, 'pres': 1, 'watch': 1, 'conduct': 1, 'machine': 1, 'light': 1, 'diagnose': 1, 'search': 1, 'complete': 1, 'reach': 1, 'sever': 1, 'encrypt': 1, 'draw': 1, 'Ad': 1, 'finish': 1, 'illuminate': 1, 'connect': 1, 'decrease': 1, 'crate': 1, 'block': 1, 'call': 1, 'time-travel': 1, 'sequence': 1, 'hail': 1, 'hide': 1, 'release': 1, 'produce': 1, 'assemble': 1, 'demonstrate': 1, 'hear': 1, 'associate': 1, 'try': 1, 'belong': 1, 'cool': 1, 'place': 1, 'estimate': 1, 'toil': 1, 'cure': 1, 'injure': 1, 'liberate': 1, 'distinguish': 1, 'ease': 1, 'facilitate': 1, 'cultivate': 1, 'blast': 1, 'begin': 1, 'reclaim': 1, 'sculpt': 1, 'tap': 1, 'staple': 1, 'teach': 1, 'achieve': 1, 'pull': 1, 'transform': 1, 'attend': 1, 'pas': 1, 'clean.': 1, 'install': 1, 'provide': 1, 'till': 1, 'screen': 1, 'conquer': 1, 'inject': 1, 'water': 1, 'set': 1, 'furnish': 1, 'purchase': 1, 'aid': 1, 'repair': 1, 'cope': 1, 'split': 1, 'fit': 1, 'deliver': 1, 'link': 1, 'in': 1, 'pick': 1, 'compare': 1, 'feather': 1, 'fry': 1, 'generate': 1, 'admit': 1, 'design': 1, 'punch': 1, 'learn': 1, 'cook': 1, 'stay': 1, 'attempt': 1, 'stand': 1, 'swim': 1, 'cut': 1, 'rebuild': 1, 'label': 1, 'capture': 1, 'execute': 1, 'interpret': 1, 'regulate': 1, 'solve': 1, 'construct': 1, 'queue': 1, 'destroy': 1, 'chase': 1, 'protect': 1, 'leave': 1, 'kick': 1, 'prefer': 1, 'join': 1, 'combine': 1, 'hit': 1, 'feature': 1, 'enable': 1, 'accomplish': 1, 'prepare': 1, 'utilize': 1, 'sell': 1, 'squeeze': 1, 'crush': 1, 'pound': 1, 'insert': 1, 'keep': 1, 'fight': 1, 'raise': 1, 'grab': 1, 'permit': 1, 'down': 1, 'throw': 1, 'buck': 1}\n",
    "\n",
    "CE = {'cause': 423, 'be': 73, 'result': 30, 'trigger': 27, 'come': 26, 'produce': 21, 'generate': 18, 'lead': 15, 'get': 11, 'make': 11, 'radiate': 11, 'reduce': 9, 'emit': 8, 'have': 6, 'instigate': 6, 'i': 5, 'arise': 5, 'catch': 5, 'start': 5, 'experience': 5, 'prevent': 5, 'stem': 5, 'create': 5, 'establish': 4, 'Cause': 4, 'give': 4, 'derive': 4, 'sell': 4, 'treat': 4, 'repair': 4, 'suffer': 4, 'turn': 3, 'contribute': 3, 'originate': 3, 'relieve': 3, 'ensue': 3, 'consider': 3, 'identify': 3, 'develop': 3, 'hit': 3, 'face': 3, 'reveal': 3, 'feel': 3, 'contain': 2, 'affect': 2, 'regard': 2, 'responsible': 2, 'spill': 2, 'scream': 2, 'resolve': 2, 'destroy': 2, 'describe': 2, 'alleviate': 2, 'gain': 2, 'root': 2, 'put': 2, 'set': 2, 'show': 2, 'help': 2, 'increase': 2, 'withdraw': 2, 'fear': 2, 'recognize': 2, 'follow': 2, 'br': 2, 'reach': 2, 'provide': 2, 'minimize': 2, 'find': 2, 'threaten': 2, 'trigger.': 1, 'sw': 1, 'close': 1, 'preclude': 1, 'release': 1, 'mark': 1, 'cau': 1, 'offer': 1, 'enter': 1, 'convert': 1, 'acknowledge': 1, 'push': 1, 'avoid': 1, 'transmit': 1, 'cite': 1, 'source': 1, 'strike': 1, 'take': 1, 'burst': 1, 'achieve': 1, 'correct': 1, 'account': 1, 'inspect': 1, 'entitle': 1, 'relate': 1, 'output': 1, 'attribut': 1, 'ensure': 1, 'move': 1, 'play': 1, 'become': 1, 'fall': 1, 'combat': 1, 'suffocation': 1, 'cause\\n': 1, 'stimulate': 1, 'transact': 1, 'visit': 1, 'depend': 1, 'reverse': 1, 'run': 1, 'direct': 1, 'make.': 1, 'tug': 1, 'detach': 1, 'eat': 1, 'conceptualize': 1, 'neutralize': 1, 'arrive': 1, 'proclaim': 1, 'initiate': 1, 'read': 1, 'survive': 1, 'shape': 1, 'asses': 1, 'unite': 1, 'prepare': 1, 'know': 1, 'translate': 1, 'craft': 1, 'deprive': 1, 'retain': 1, 'mov': 1, 'decide': 1, 'support': 1, 'eliminate': 1, 'depart': 1, 'cough': 1, 'boost': 1, 'grow': 1, 'weaken': 1, 'reside': 1, 'consist': 1, 'batter': 1, 'send': 1, 'expose': 1, 'succumb': 1, 'share': 1, 'burn': 1, 'overstate': 1, 'accommodate': 1, 'contract': 1, 'check': 1, 'inundate': 1, 'donate': 1, 'end': 1, 'Place': 1, 'offset': 1, 'spread': 1, 'fill': 1, 'subject': 1, 'intubation': 1, 'receive': 1, 'essential': 1, 'mitigate': 1, 'choke': 1, 'await': 1, 'quench': 1, 'change': 1, 'improve': 1, 'see': 1, 'teach': 1, 'enhance': 1, 'sale_of': 1, 'resist': 1, 'drink': 1, 'smile': 1, 'demonstrate': 1, 'happen': 1, 'occur': 1, 'bombard': 1, 'include': 1, 'allow': 1, 'delay': 1, 'sleep': 1, 'respond': 1, 'block': 1, 'brace': 1, 'producean': 1, 'after': 1, 'obtain': 1, 'examine': 1, 'mediate': 1, 'emanate': 1, 'consumption': 1, 'beget': 1, 'utter': 1, 'connect': 1, 'counteract': 1}\n",
    "\n",
    "MC = {'be': 77, 'have': 27, 'find': 11, 'consist': 11, 'join': 8, 'bring': 8, 'come': 8, 'contain': 7, 'become': 6, 'make': 6, 'take': 6, 'form': 6, 'establish': 6, 'attack': 5, 'build': 5, 'stand': 5, 'live': 5, 'see': 5, 'develop': 5, 'comprise': 4, 'drive': 4, 'gather': 4, 'compose': 4, 'start': 4, 'produce': 4, 'spot': 4, 'send': 4, 'create': 4, 'fly': 4, 'include': 3, 'call': 3, 'serve': 3, 'keep': 3, 'get': 3, 'study': 3, 'leave': 3, 'arrive': 3, 'fall': 3, 'select': 3, 'organize': 3, 'maintain': 3, 'watch': 3, 'lead': 3, 'provide': 3, 'show': 3, 'perform': 3, 'support': 3, 'work': 3, 'collect': 2, 'pull': 2, 'move': 2, 'pas': 2, 'swim': 2, 'observe': 2, 'reflect': 2, 'investigate': 2, 'increase': 2, 'migrate': 2, 'splash': 2, 'prove': 2, 'dwell': 2, 'recapture': 2, 'employ': 2, 'turn': 2, 'use': 2, 'play': 2, 'pick': 2, 'purchase': 2, 'carry': 2, 'return': 2, 'meet': 2, 'describe': 2, 'sign': 2, 'expose': 2, 'sell': 2, 'i': 2, 'represent': 2, 'fee': 2, 'involve': 2, 'issue': 2, 'set': 2, 'follow': 2, 'harbor': 2, 'announce': 2, 'descend': 2, 'face': 2, 'train': 2, 'associate': 2, 'decide': 2, 'mobilize': 2, 'fight': 2, 'constitute': 2, 'witnes': 2, 'hire': 2, 'surround': 2, 'drift': 2, 'handle': 2, 'shake': 2, 'enter': 2, 'look': 2, 'back': 1, 'amount': 1, 'lie': 1, 'degenerate': 1, 'arm': 1, 'complement': 1, 'challenge': 1, 'Pas': 1, 'offer': 1, 'direct': 1, 'bear': 1, 'visit': 1, 'lure': 1, 'tremble': 1, 'exposure': 1, 'raise': 1, 'organise': 1, 'board': 1, 'attend': 1, 'perch': 1, 'hear': 1, 'marvell': 1, 'impres': 1, 'assemble': 1, 'lay': 1, 'continue': 1, 'plague': 1, 'enroll': 1, 'cause': 1, 'deploy': 1, 'cros': 1, 'pay': 1, 'manage': 1, 'encounter': 1, 'skulk': 1, 'identify': 1, 'member': 1, 'feel': 1, 'storm': 1, 'abandon': 1, 'devise': 1, 'wait': 1, 'serenade': 1, 'dash': 1, 'secure': 1, 'fill': 1, 'cl': 1, 'take.': 1, 'float': 1, 'bask': 1, 'choose': 1, 'plow': 1, 'experience': 1, 'chair': 1, 'stack': 1, 'arrest': 1, 'receive': 1, 'reconstitute': 1, 'depict': 1, 'connect': 1, 'initiate': 1, 'say': 1, 'draw': 1, 'coo': 1, 'focu': 1, 'reduce': 1, 'presid': 1, 'loan': 1, 'scavenge': 1, 'run': 1, 'bellow': 1, 'attract': 1, 'embrace': 1, 'feast': 1, 'glid': 1, 'Interact': 1, 'burst': 1, 'Overrun': 1, 'resign': 1, 'vote': 1, 'drop': 1, 'exist': 1, 'trot': 1, 'sire': 1, 'entertain': 1, 'seize': 1, 'share': 1, 'suspend': 1, 'change': 1, 'examine': 1, 'forge': 1, 'close': 1, 'appear': 1, 'contend': 1, 'infest': 1, 'reteam': 1, 'struggle': 1, 'defeat': 1, 'conquer': 1, 'invest': 1, 'rent': 1, 'cadge': 1, 'hang': 1, 'acquir': 1, 'obtain': 1, 'assert': 1, 'point': 1, 'hunt': 1, 'rope': 1, 'integrate': 1, 'ease': 1, 'place': 1, 'discus': 1, 'emerge': 1, 'rail': 1, 'remind': 1, 'fit': 1, 'teach': 1, 'depend': 1, 'replace': 1, 'bind': 1, 'notice': 1, 'deny': 1, 'cover': 1, 'jostl': 1, 'charm': 1, 'arrange': 1, 'skip': 1, 'comment': 1, 'crouch': 1, 'sound': 1, 'abduct': 1, 'aim': 1, 'Br': 1, 'accompany': 1, 'assault': 1, 'cast': 1, 'destroy': 1, 'ingest': 1, 'begin': 1, 'rely': 1, 'stare': 1, 'grant': 1, 'speak': 1, 'appoint': 1, 'party': 1, 'portray': 1, 'stay': 1, 'devote': 1, 'order': 1, 'soar': 1, 'purr': 1, 'auction': 1, 'expect': 1, 'sacrifice': 1, 'result': 1, 'battle': 1, 'belong': 1, 'advocate': 1, 'contribute': 1, 'pit': 1, 'betray': 1, 'catch': 1, 'recall': 1, 'tumble': 1, 'tease': 1, 'yell': 1, 'tell': 1, 'march': 1, 'promote': 1, 'contract': 1, 'feature': 1, 'step': 1, 'brave': 1, 'resemble': 1, 'reject': 1, 'nest': 1, 'shift': 1, 'deal': 1, 'stumble': 1, 'imagine': 1, 'name': 1, 'scurry': 1, 'convene': 1, 'generate': 1, 'motivat': 1, 'adapt': 1, 'try': 1, 'undulate': 1, 'conceptualize': 1, 'discover': 1, 'swarm': 1, 'read': 1, 'season': 1, 'appeal': 1, 'realize': 1, 'drown': 1, 'unfold': 1, 'populate': 1, 'trigger': 1, 'rescue': 1, 'sight': 1, 'mark': 1, 'supply': 1, 'confirm': 1, 'walk': 1, 'undertake': 1, 'detect': 1, 'survive': 1, 'latch': 1, 'traverse': 1, 'convert': 1, 'flee': 1, 'satisfy': 1, 'gallop': 1, 'erupt': 1, 'utilization': 1, 'advance': 1, 'locate': 1, 'sail': 1, 'Picture': 1, 'exhume': 1, 'publish': 1, 'approve': 1, 'compete': 1, 'predicate': 1, 'evaluate': 1, 'trim': 1, 'let': 1, 'launch': 1, 'break': 1, 'scold': 1, 'indicate': 1, 'explain': 1, 'whirl': 1, 'fortify': 1, 'bounce': 1, 'gamble': 1, 'chase': 1, 'prepare': 1, 'evince': 1, 'cal': 1, 'breathe': 1}\n",
    "\n",
    "MT = {'be': 48, 'describe': 20, 'make': 18, 'discus': 16, 'reflect': 15, 'explain': 13, 'relate': 12, 'have': 10, 'depict': 10, 'contain': 10, 'report': 10, 'give': 9, 'provide': 9, 'explore': 9, 'investigate': 8, 'point': 8, 'examine': 8, 'illustrate': 7, 'define': 7, 'inform': 7, 'document': 7, 'advertise': 6, 'center': 6, 'declare': 6, 'concern': 6, 'outline': 6, 'focu': 6, 'become': 6, 'deal': 6, 'talk': 5, 'form': 5, 'publish': 5, 'supply': 5, 'cover': 5, 'summarise': 5, 'consider': 5, 'set': 5, 'narrate': 4, 'offer': 4, 'devote': 4, 'apply': 4, 'show': 4, 'study': 4, 'criticize': 4, 'run': 4, 'tell': 4, 'speak': 4, 'feature': 4, 'develop': 4, 'comment': 4, 'detail': 4, 'receive': 4, 'introduce': 4, 'come': 4, 'tackle': 4, 'highlight': 4, 'appear': 4, 'mention': 3, 'analyse': 3, 'prepare': 3, 'include': 3, 'find': 3, 'release': 3, 'assert': 3, 'swirl': 3, 'learn': 3, 'create': 3, 'adopt': 3, 'addres': 3, 'involve': 3, 'compare': 3, 'reveal': 3, 'survey': 3, 'state': 3, 'remind': 2, 'convene': 2, 'analyze': 2, 'obtain': 2, 'promote': 2, 'raise': 2, 'produce': 2, 'hold': 2, 'portray': 2, 'use': 2, 'teach': 2, 'send': 2, 'insist': 2, 'criticise': 2, 'take': 2, 'go': 2, 'review': 2, 'base': 2, 'chart': 2, 'conduct': 2, 'present': 2, 'pertain': 2, 'encourage': 2, 'establish': 2, 'govern': 2, 'accept': 2, 'expres': 2, 'write': 2, 'allege': 2, 'stimulate': 2, 'br': 2, 'conclude': 2, 'issue': 1, 'construct': 1, 'incorporate': 1, 'launch': 1, 'respond': 1, 'Be': 1, 'inquire': 1, 'comprise': 1, 'represent': 1, 'emphasise': 1, 'serve': 1, 'rotate': 1, 'achieve': 1, 'differ': 1, 'provoke': 1, 'submit': 1, 'engage': 1, 'begin': 1, 'indicate': 1, 'witnes': 1, 'pas': 1, 'recount': 1, 'attribute': 1, 'summarize': 1, 'furnish': 1, 'dominate': 1, 'surround': 1, 'enter': 1, 'keep': 1, 'solve': 1, 'approache': 1, 'discusse': 1, 'trace': 1, 'approach': 1, 'direct': 1, 'suspend': 1, 'retrieve': 1, 'drive': 1, 'consider.': 1, 'reach': 1, 'emphasize': 1, 'add': 1, 'evolve': 1, 'erupt': 1, 'meet': 1, 'ad': 1, 'paint': 1, 'capture': 1, 'facilitate': 1, 'say': 1, 'occur': 1, 'pronounce': 1, 'demand': 1, 'design.': 1, 'need': 1, 'tabulate': 1, 'propose': 1, 'specialize': 1, 'answer': 1, 'move': 1, 'get': 1, 'sw': 1, 'refer': 1, 'open': 1, 'limit': 1, 'look': 1, 'make.': 1, 'dedicate': 1, 'explore.': 1, 'organise': 1, 'regulate': 1, 'piece': 1, 'position': 1, 'connect': 1, 'characterize': 1, 'increase': 1} \n",
    "\n",
    "ED = {'place': 56, 'put': 53, 'move': 44, 'pour': 39, 'send': 34, 'migrate': 27, 'release': 27, 'add': 21, 'spread': 20, 'deliver': 19, 'insert': 19, 'arrive': 19, 'drop': 18, 'drain': 16, 'import': 15, 'throw': 15, 'blow': 14, 'invest': 14, 'travel': 13, 'donate': 13, 'transport': 12, 'take': 12, 'stuff': 11, 'inject': 11, 'pas': 11, 'export': 10, 'leak': 10, 'enter': 10, 'carry': 9, 'push': 9, 'run': 9, 'flow': 9, 'br': 9, 'go': 8, 'dump': 8, 'drag': 8, 'land': 8, 'journey': 8, 'implant': 8, 'fetch': 7, 'give': 7, 'ship': 7, 'hand': 7, 'fly': 6, 'remove': 6, 'fall': 6, 'reschedule': 6, 'misplace': 6, 'sink': 5, 'approach': 5, 'postpone': 4, 'pack': 4, 'award': 4, 'be': 4, 'enclose': 4, 'seal': 3, 'post': 3, 'store': 3, 'Pour': 3, 'bestow': 3, 'lock': 2, 'start': 2, 'come': 2, 'collect': 2, 'install': 2, 'load': 1, 'incorporate': 1, 'slip': 1, 'inherit': 1, 'transfer': 1, 'deposit': 1, 'Move': 1, 'sail': 1, 'retract': 1, 'stir': 1, 'mix': 1, 'locate': 1, 'boost': 1, 'Add': 1, 'hide': 1, 'arriv': 1, 'rescue': 1, 'carve': 1, 'remain': 1, 'spool': 1, 'fill': 1, 'look': 1, 'overcharge': 1, 'supply': 1, 'Drain': 1, 'crash': 1, 'bring': 1, 'beat': 1, 'meet': 1, 'hold': 1, 'show': 1, 'bank': 1, 'gather': 1, 'filter': 1, 'Take': 1, 'relieve': 1, 'concern': 1, 'change': 1, 'include': 1, 'bury': 1, 'spend': 1, 'transmit': 1, 'grant': 1, 'leak.': 1, 'extractant': 1, 'integrate': 1, 'introduce': 1, 'stash': 1, 'make': 1, 'identify': 1}\n",
    "\n",
    "CW = {'be': 94, 'have': 55, 'contain': 33, 'comprise': 30, 'include': 26, 'make': 22, 'hold': 12, 'consist': 12, 'compose': 12, 'use': 11, 'show': 11, 'attach': 10, 'place': 8, 'look': 7, 'connect': 7, 'provide': 7, 'move': 7, 'find': 7, 'develop': 6, 'open': 6, 'work': 6, 'appear': 6, 'br': 6, 'i': 6, 'feature': 6, 'form': 6, 'start': 6, 'take': 6, 'break': 5, 'click': 5, 'rotate': 5, 'set': 5, 'pres': 5, 'locate': 5, 'extend': 4, 'design': 4, 'construct': 4, 'carve': 4, 'display': 4, 'combine': 4, 'remove': 4, 'fit': 4, 'store': 4, 'present': 4, 'create': 4, 'keep': 4, 'mount': 4, 'fill': 4, 'see': 4, 'enclose': 4, 'divide': 3, 'result': 3, 'stand': 3, 'require': 3, 'rest': 3, 'decorate': 3, 'depend': 3, 'grow': 3, 'publish': 3, 'increase': 3, 'clean': 3, 'allow': 3, 'hang': 3, 'reflect': 3, 'accommodate': 3, 'run': 3, 'install': 3, 'measure': 3, 'push': 3, 'build': 3, 'perform': 3, 'involve': 3, 'reduce': 2, 'report': 2, 'reach': 2, 'regulate': 2, 'insert': 2, 'notice': 2, 'anchor': 2, 'lock': 2, 'wrap': 2, 'paint': 2, 'fix': 2, 'carry': 2, 'stretch': 2, 'come': 2, 'receive': 2, 'relate': 2, 'hit': 2, 'wear': 2, 'grab': 2, 'detach': 2, 'pull': 2, 'lift': 2, 'drag': 2, 'summarise': 2, 'touch': 2, 'support': 2, 'proces': 2, 'force': 2, 'draw': 2, 'feel': 2, 'lie': 2, 'operate': 2, 'transmit': 2, 'catch': 2, 'check': 2, 'walk': 2, 'launch': 2, 'compris': 2, 'judge': 2, 'pas': 2, 'correspond': 2, 'release': 2, 'give': 2, 'cause': 2, 'follow': 2, 'fall': 2, 'mark': 2, 'put': 2, 'power': 2, 'go': 2, 'eliminate': 1, 'pad': 1, 'stop': 1, 'leave': 1, 'Unwind': 1, 'fly': 1, 'span': 1, 'permit': 1, 'collapse': 1, 'recur': 1, 'get': 1, 'filter': 1, 'riffle': 1, 'control': 1, 'list': 1, 'zero': 1, 'engage': 1, 'account': 1, 'arrange': 1, 'unscrew': 1, 'shape': 1, 'probe': 1, 'choose': 1, 'prim': 1, 'shorten': 1, 'couple': 1, 'offer': 1, 'undertake': 1, 'snorgle': 1, 'flare': 1, 'lead': 1, 'unlock': 1, 'hatchet': 1, 'date': 1, 'scale': 1, 'curl': 1, 'strike': 1, 'cultivate': 1, 'dedicate': 1, 'survive': 1, 'load': 1, 'raise': 1, 'throw': 1, 'function': 1, 'play': 1, 'consider': 1, 'disregard': 1, 'vibrate': 1, 'expenditure': 1, 'guide': 1, 'signal': 1, 'cut': 1, 'Embroidered.': 1, 'pierc': 1, 'settle': 1, 'fee': 1, 'Recognized': 1, 'Point': 1, 'maximize': 1, 'pry': 1, 'circulate': 1, 'rig': 1, 'twist': 1, 'modify': 1, 'depict': 1, 'drive': 1, 'expos': 1, 'absorb': 1, 'shear': 1, 'hide': 1, 'collect': 1, 'roam': 1, 'scrape': 1, 'threaten': 1, 'position': 1, 'fire': 1, 'complement': 1, 'slice': 1, 'clasp': 1, 'become': 1, 'coupl': 1, 'observe': 1, 'stab': 1, 'pop': 1, 'stun': 1, 'protect': 1, 'bend': 1, 'curtain': 1, 'record': 1, 'grip': 1, 'redesign': 1, 'narrate': 1, 'help': 1, 'convince': 1, 'spin': 1, 'gleam': 1, 'malfunction': 1, 'swim': 1, 'lubricate': 1, 'change': 1, 'adjust': 1, 'slip': 1, 'root': 1, 'addres': 1, 'search': 1, 'remember': 1, 'scatter': 1, 'manufacture': 1, 'occur': 1, 'cleat': 1, 'thrust': 1, 'adopt': 1, 'grasp': 1, 'cover': 1, 'swish': 1, 'foresee': 1, 'calculate': 1, 'lower': 1, 'perfect': 1, 'refer': 1, 'begin': 1, 'heal': 1, 'paddle': 1, 'retract': 1, 'preside': 1, 'reveal': 1, 'post': 1, 'mean': 1, 'stick': 1, 'study': 1, 'graz': 1, 'kis': 1, 'opt': 1, 'cock': 1, 'beat': 1, 'contribute': 1, 'align': 1, 'flow': 1, 'flood': 1, 'purchase': 1, 'Assist': 1, 'insulate': 1, 'sense': 1, 'heat': 1, 'expand': 1, 'convert': 1, 'base': 1, 'float': 1, 'mix': 1, 'stay': 1, 'meet': 1, 'capture': 1, 'maintain': 1, 'save': 1, 'prepare': 1, 'withstand': 1, 'complain': 1, 'situate': 1, 'lose': 1, 'close': 1, 'improve': 1, 'wax': 1, 'eat': 1, 'detect': 1, 'clip': 1, 'belong': 1, 'fasten': 1, 'predict': 1, 'weaken': 1, 'scrub': 1, 'vary': 1, 'inspect': 1, 'publish.': 1, 'ascend': 1, 'scratch': 1, 'plant': 1, 'overhang': 1, 'average': 1, 'acces': 1, 'dangle': 1, 'sw': 1, 'lean': 1, 'tune': 1, 'thicken': 1, 'locate.': 1, 'prevent': 1, 'solder': 1, 'produce': 1, 'ha': 1, 'empty': 1, 'appreciate': 1, 'love': 1, 'procure': 1, 'hatch': 1, 'secrete': 1, 'block': 1, 'read': 1, 'behave': 1, 'explore': 1, 'erode': 1, 'gain': 1, 'recognize': 1, 'digest': 1, 'track': 1, 'collaborate': 1, 'analyze': 1, 'seek': 1, 'reinforce': 1, 'rise': 1, 'blink': 1, 'Combine': 1, 'step': 1, 'bounce': 1, 'crush': 1, 'delete': 1, 'outfit': 1, 'enter': 1, 'add': 1, 'consume': 1, 'share': 1, 'assist': 1, 'drape': 1}\n",
    "\n",
    "CC = {'find': 24, 'store': 22, 'enclose': 15, 'discover': 14, 'hide': 12, 'keep': 12, 'be': 11, 'lock': 10, 'hold': 10, 'contain': 8, 'include': 8, 'carry': 7, 'wa': 7, 'put': 6, 'have': 5, 'seal': 4, 'give': 4, 'make': 4, 'br': 3, 'i': 3, 'place': 3, 'send': 3, 'sit': 3, 'contained.': 3, 'show': 3, 'full': 3, 'see': 3, 'eat': 2, 'strike': 2, 'serve': 2, 'carried.': 2, 'get': 2, 'take': 2, 'fit': 2, 'in': 2, 'pick': 2, 'cover': 2, 'open': 2, 'come': 2, 'tuck': 2, 'use': 2, 'cook': 2, 'bring': 2, 'goe': 1, 'separate': 1, 'preserve': 1, 'coil': 1, 'stor': 1, 'scratch': 1, 'secure': 1, 'arrive': 1, 'fold': 1, 'intend': 1, 'introduce': 1, 'deliver': 1, 'hang': 1, 'receive': 1, 'throw': 1, 'insert': 1, 'weigh': 1, 'prefer': 1, 'kill': 1, 'bear': 1, 'label': 1, 'attach': 1, 'return': 1, 'inside': 1, 'connect': 1, 'Inside': 1, 'found.': 1, 'mis': 1, 'cost': 1, 'folder': 1, 'print': 1, 'pull': 1, 'unpack': 1, 'fill': 1, 'stand': 1, 'search': 1, 'plac': 1, 'accept': 1, 'sup': 1, 'fix': 1, 'traverse': 1, 'filled\\twith': 1, 'stir': 1, 'comprise': 1, 'measure': 1, 'dissolve': 1, 'inventoried': 1, 'press': 1, 'house': 1, 'Mak': 1, 'examine': 1, 'leave': 1, 'tak': 1, 'measur': 1, 'explode': 1, 'talk': 1, 'pas': 1, 'become': 1, 'smuggle': 1, 'sleep': 1, 'look': 1, 'snatch': 1, 'arrange': 1, 'remove': 1, 'offer': 1, 'cram': 1, 'mark': 1, 'face': 1, 'ship': 1, 'try': 1, 'impart': 1, 'alert': 1, 'stored.': 1, 'spill': 1, 'protect': 1, 'conceal': 1, 'saw': 1, 'fly': 1, 'deposit': 1, 'play': 1, 'escape': 1, 'lay.': 1, 'buy': 1, 'display': 1, 'market': 1, 'enter': 1, 'seize': 1, 'check': 1, 'grow': 1, 'mistake': 1, 'notice': 1}\n",
    "\n",
    "EO = {'be': 86, 'contain': 65, 'find': 29, 'store': 23, 'enclose': 19, 'discover': 14, 'lock': 12, 'keep': 11, 'hide': 10, 'place': 9, 'carry': 9, 'put': 9, 'hold': 9, 'wa': 8, 'have': 6, 'br': 5, 'include': 5, 'seal': 4, 'fill': 4, 'make': 4, 'serve': 3, 'give': 3, 'sit': 3, 'cover': 3, 'open': 3, 'see': 3, 'eat': 2, 'i': 2, 'wrap': 2, 'send': 2, 'fit': 2, 'measure': 2, 'pull': 2, 'tuck': 2, 'use': 2, 'determine': 2, 'hav': 2, 'container': 2, 'explode': 2, '\"contains\"': 2, 'leave': 2, 'remove': 2, 'Wa': 2, 'go': 1, 'date': 1, 'preserve': 1, 'lose': 1, 'strike': 1, 'coil': 1, 'stor': 1, 'scratch': 1, 'secure': 1, 'arrive': 1, 'attach': 1, 'introduce': 1, 'symbolise': 1, 'hang': 1, 'receive': 1, 'throw': 1, 'weigh': 1, 'kill': 1, 'label': 1, 'fall': 1, 'pour': 1, 'note': 1, 'connect': 1, 'cost': 1, '\"shows\"': 1, 'print': 1, 'unpack': 1, 'stand': 1, 'take': 1, 'full': 1, 'own': 1, 'accept': 1, 'rattle': 1, 'drink': 1, 'ensure': 1, 'cease': 1, 'ha': 1, 'stir': 1, 'ensconce': 1, 'dissolve': 1, 'generate': 1, 'inventory': 1, 'house': 1, 'examine': 1, 'tak': 1, 'inside': 1, 'Find': 1, '\"inside\"': 1, 'bury': 1, 'Remember': 1, 'show': 1, 'pas': 1, 'auction.': 1, 'turn': 1, 'become': 1, 'See': 1, 'sleep': 1, 'stuff': 1, 'snatch': 1, 'arrange': 1, 'offer': 1, 'cram': 1, 'retriev': 1, 'ship': 1, 'try': 1, 'impart': 1, 'bind': 1, 'protect': 1, 'conceal': 1, 'fly': 1, 'get': 1, 'deposit': 1, 'lock.': 1, 'play': 1, 'escape': 1, 'display': 1, 'come': 1, 'enter': 1, 'sort': 1, 'seize': 1, 'check': 1, 'grow': 1, 'mistake': 1, 'Notice': 1, 'surround': 1}\n",
    "\n",
    "PP = {'be': 103, 'come': 35, 'make': 25, 'derive': 22, 'leave': 21, 'arrive': 20, 'originate': 20, 'run': 15, 'have': 13, 'take': 12, 'distill': 12, 'farm': 11, 'produce': 10, 'release': 10, 'go': 10, 'descend': 9, 'fall': 9, 'extract': 8, 'manufacture': 8, 'start': 7, 'obtain': 7, 'emerge': 7, 'remove': 6, 'pop': 6, 'use': 5, 'get': 5, 'root': 4, 'move': 4, 'leak': 4, 'abscond': 4, 'hail': 3, 'evolve': 3, 'develop': 3, 'add': 3, 'evaporate': 3, 'elude': 3, 'send': 3, 'br': 3, 'form': 3, 'cook': 3, 'involve': 3, 'include': 3, 'put': 3, 'generate': 3, 'sail': 3, 'drink': 3, 'gain': 2, 'arise': 2, 'flourish': 2, 'grow': 2, 'become': 2, 'collect': 2, 'look': 2, 'found.': 2, 'break': 2, 'taxi': 2, 'date': 2, 'try': 2, 'pour': 2, 'taste': 2, 'refer': 2, 'describe': 2, 'travel': 2, 'feature': 2, 'bail': 2, 'construct': 2, 'emanate': 2, 'gather': 2, 'see': 2, 'give': 2, 'treat': 2, 'receive': 2, 'alarm': 1, 'distil': 1, 'yield': 1, 'drive': 1, 'bloom': 1, 'escape': 1, 'act': 1, 'delve': 1, 'tear': 1, 'identify': 1, 'deliver': 1, 'sequence': 1, 'reach': 1, 'fire': 1, 'fund': 1, 'depart': 1, 'create': 1, 'steal.': 1, 'radiate': 1, 'achieve': 1, 'end': 1, 'win': 1, 'represent': 1, 'consider': 1, 'infiltrate': 1, 'organize': 1, 'follow': 1, 'deice': 1, 'attribute': 1, 'juxtapose': 1, 'strengthen': 1, 'flee': 1, 'prove': 1, 'combine': 1, 'emerge.': 1, 'recall': 1, 'exit': 1, 'twist': 1, 'update': 1, 'graduate': 1, 'convert': 1, 'save': 1, 'mean': 1, 'set': 1, 'return': 1, 'offer': 1, 'display': 1, 'supervise': 1, 'draw': 1, 'estimate': 1, 'review': 1, 'serve': 1, 'respond': 1, 'explain': 1, 'learn': 1, 'reveal': 1, 'consult': 1, 'order': 1, 'pres': 1, 'play': 1, 'climb': 1, 'base': 1, 'direct': 1, 'compose': 1, 'push': 1, 'be_given': 1, 'understand': 1, 'pull': 1, 'resign': 1, 'incorporate': 1, 'eject': 1, 'submit': 1, 'stimulate': 1, 'copy': 1, 'be_surprised': 1, 'solicit': 1, 'provide': 1, 'watch': 1, 'switch': 1, 'evacuate': 1, 'find.': 1, 'i': 1, 'depend': 1, 'qualify': 1, 'say': 1, 'investigate': 1, 'sample': 1, 'catch': 1, 'throw': 1, 'group': 1, 'cure': 1, 'fuel': 1, 'popularize': 1, 'begin': 1, 'bake': 1, 'step': 1, 'notify': 1, 'star': 1, 'drown.': 1, 'mention': 1, 'reschedule': 1, 'hand': 1, 'uprise': 1, 'lie': 1, 'kick': 1, 'leap': 1, 'sweeten': 1, 'stem': 1, 'secure': 1, 'remain': 1, 'demolish.': 1, 'replicate': 1, 'find': 1, 'better': 1, 'lodge': 1, 'retrieve': 1, 'share': 1, 'clog': 1, 'attract': 1, 'perform': 1, 'work': 1, 'meet': 1, 'report': 1, 'steal': 1, 'harm': 1, 'recruit': 1, 'compile': 1, 'outline': 1, 'cause': 1, 'prepare': 1, 'Reject': 1}\n",
    "\n",
    "DICTS = [(IA, 'IA'), (PP, 'PP'), (CE, 'CE'), (MC, 'MC'),\n",
    "         (EO, 'EO'), (MT, 'MT'), (ED, 'ED'), (CW, 'CW'), (CC, 'CC')]\n",
    "\n",
    "\n",
    "LOG_DICTS = []\n",
    "for d in DICTS:\n",
    "    LOG_DICTS.append(({key: math.log10(value)\n",
    "                     for key, value in d[0].items()}, d[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe a better approach is to softmax the dict values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'use': 1.0, 'apply': 8.75651076269652e-27, 'take': 4.359610000063081e-28, 'be': 2.1705220113036395e-29, 'attach': 7.984904245686979e-30, 'create': 7.984904245686979e-30, 'wield': 7.984904245686979e-30, 'kill': 2.9374821117108028e-30, 'power': 3.975449735908647e-31, 'play': 1.462486227251231e-31, 'clean': 1.462486227251231e-31, 'build': 1.462486227251231e-31, 'form': 1.462486227251231e-31, 'manipulate': 1.462486227251231e-31, 'drive': 1.462486227251231e-31, 'carry': 5.380186160021138e-32, 'work': 5.380186160021138e-32, 'fix': 5.380186160021138e-32, 'mark': 5.380186160021138e-32, 'give': 5.380186160021138e-32, 'turn': 5.380186160021138e-32, 'adjust': 5.380186160021138e-32, 'show': 5.380186160021138e-32, 'perform': 5.380186160021138e-32, 'scale': 5.380186160021138e-32, 'find': 1.9792598779469045e-32, 'stir': 1.9792598779469045e-32, 'choose': 1.9792598779469045e-32, 'acces': 1.9792598779469045e-32, 'determine': 1.9792598779469045e-32, 'treat': 1.9792598779469045e-32, 'tear': 1.9792598779469045e-32, 'fly': 1.9792598779469045e-32, 'threaten': 1.9792598779469045e-32, 'stop': 1.9792598779469045e-32, 'strike': 1.9792598779469045e-32, 'proces': 1.9792598779469045e-32, 'have': 1.9792598779469045e-32, 'start': 1.9792598779469045e-32, 'br': 1.9792598779469045e-32, 'move': 7.281290178321643e-33, 'implement': 7.281290178321643e-33, 'break': 7.281290178321643e-33, 'calibrate': 7.281290178321643e-33, 'approach': 7.281290178321643e-33, 'resolve': 7.281290178321643e-33, 'assist': 7.281290178321643e-33, 'write': 7.281290178321643e-33, 'cover': 7.281290178321643e-33, 'identify': 7.281290178321643e-33, 'study': 7.281290178321643e-33, 'make': 7.281290178321643e-33, 'touch': 7.281290178321643e-33, 'help': 7.281290178321643e-33, 'bake': 7.281290178321643e-33, 'offer': 7.281290178321643e-33, 'improve': 7.281290178321643e-33, 'carve': 7.281290178321643e-33, 'employ': 7.281290178321643e-33, 'receive': 7.281290178321643e-33, 'buy': 7.281290178321643e-33, 'come': 7.281290178321643e-33, 'win': 7.281290178321643e-33, 'relax': 7.281290178321643e-33, 'wear': 7.281290178321643e-33, 'eat': 7.281290178321643e-33, 'push': 7.281290178321643e-33, 'put': 7.281290178321643e-33, 'examine': 7.281290178321643e-33, 'need': 7.281290178321643e-33, 'remove': 7.281290178321643e-33, 'deface': 7.281290178321643e-33, 'obtain': 7.281290178321643e-33, 'batter': 7.281290178321643e-33, 'observe': 7.281290178321643e-33, 'hold': 7.281290178321643e-33, 'sweep': 7.281290178321643e-33, 'operate': 7.281290178321643e-33, 'travel': 7.281290178321643e-33, 'get': 7.281290178321643e-33, 'saw': 7.281290178321643e-33, 'establish': 7.281290178321643e-33, 'swap': 2.6786369618080778e-33, 'propose': 2.6786369618080778e-33, 'sew': 2.6786369618080778e-33, 'peel': 2.6786369618080778e-33, 'benefit': 2.6786369618080778e-33, 'plow': 2.6786369618080778e-33, 'catch': 2.6786369618080778e-33, 'drink': 2.6786369618080778e-33, 'filter': 2.6786369618080778e-33, 'locate': 2.6786369618080778e-33, 'asses': 2.6786369618080778e-33, 'mix': 2.6786369618080778e-33, 'date': 2.6786369618080778e-33, 'coat': 2.6786369618080778e-33, 'regain': 2.6786369618080778e-33, 'pres': 2.6786369618080778e-33, 'watch': 2.6786369618080778e-33, 'conduct': 2.6786369618080778e-33, 'machine': 2.6786369618080778e-33, 'light': 2.6786369618080778e-33, 'diagnose': 2.6786369618080778e-33, 'search': 2.6786369618080778e-33, 'complete': 2.6786369618080778e-33, 'reach': 2.6786369618080778e-33, 'sever': 2.6786369618080778e-33, 'encrypt': 2.6786369618080778e-33, 'draw': 2.6786369618080778e-33, 'Ad': 2.6786369618080778e-33, 'finish': 2.6786369618080778e-33, 'illuminate': 2.6786369618080778e-33, 'connect': 2.6786369618080778e-33, 'decrease': 2.6786369618080778e-33, 'crate': 2.6786369618080778e-33, 'block': 2.6786369618080778e-33, 'call': 2.6786369618080778e-33, 'time-travel': 2.6786369618080778e-33, 'sequence': 2.6786369618080778e-33, 'hail': 2.6786369618080778e-33, 'hide': 2.6786369618080778e-33, 'release': 2.6786369618080778e-33, 'produce': 2.6786369618080778e-33, 'assemble': 2.6786369618080778e-33, 'demonstrate': 2.6786369618080778e-33, 'hear': 2.6786369618080778e-33, 'associate': 2.6786369618080778e-33, 'try': 2.6786369618080778e-33, 'belong': 2.6786369618080778e-33, 'cool': 2.6786369618080778e-33, 'place': 2.6786369618080778e-33, 'estimate': 2.6786369618080778e-33, 'toil': 2.6786369618080778e-33, 'cure': 2.6786369618080778e-33, 'injure': 2.6786369618080778e-33, 'liberate': 2.6786369618080778e-33, 'distinguish': 2.6786369618080778e-33, 'ease': 2.6786369618080778e-33, 'facilitate': 2.6786369618080778e-33, 'cultivate': 2.6786369618080778e-33, 'blast': 2.6786369618080778e-33, 'begin': 2.6786369618080778e-33, 'reclaim': 2.6786369618080778e-33, 'sculpt': 2.6786369618080778e-33, 'tap': 2.6786369618080778e-33, 'staple': 2.6786369618080778e-33, 'teach': 2.6786369618080778e-33, 'achieve': 2.6786369618080778e-33, 'pull': 2.6786369618080778e-33, 'transform': 2.6786369618080778e-33, 'attend': 2.6786369618080778e-33, 'pas': 2.6786369618080778e-33, 'clean.': 2.6786369618080778e-33, 'install': 2.6786369618080778e-33, 'provide': 2.6786369618080778e-33, 'till': 2.6786369618080778e-33, 'screen': 2.6786369618080778e-33, 'conquer': 2.6786369618080778e-33, 'inject': 2.6786369618080778e-33, 'water': 2.6786369618080778e-33, 'set': 2.6786369618080778e-33, 'furnish': 2.6786369618080778e-33, 'purchase': 2.6786369618080778e-33, 'aid': 2.6786369618080778e-33, 'repair': 2.6786369618080778e-33, 'cope': 2.6786369618080778e-33, 'split': 2.6786369618080778e-33, 'fit': 2.6786369618080778e-33, 'deliver': 2.6786369618080778e-33, 'link': 2.6786369618080778e-33, 'in': 2.6786369618080778e-33, 'pick': 2.6786369618080778e-33, 'compare': 2.6786369618080778e-33, 'feather': 2.6786369618080778e-33, 'fry': 2.6786369618080778e-33, 'generate': 2.6786369618080778e-33, 'admit': 2.6786369618080778e-33, 'design': 2.6786369618080778e-33, 'punch': 2.6786369618080778e-33, 'learn': 2.6786369618080778e-33, 'cook': 2.6786369618080778e-33, 'stay': 2.6786369618080778e-33, 'attempt': 2.6786369618080778e-33, 'stand': 2.6786369618080778e-33, 'swim': 2.6786369618080778e-33, 'cut': 2.6786369618080778e-33, 'rebuild': 2.6786369618080778e-33, 'label': 2.6786369618080778e-33, 'capture': 2.6786369618080778e-33, 'execute': 2.6786369618080778e-33, 'interpret': 2.6786369618080778e-33, 'regulate': 2.6786369618080778e-33, 'solve': 2.6786369618080778e-33, 'construct': 2.6786369618080778e-33, 'queue': 2.6786369618080778e-33, 'destroy': 2.6786369618080778e-33, 'chase': 2.6786369618080778e-33, 'protect': 2.6786369618080778e-33, 'leave': 2.6786369618080778e-33, 'kick': 2.6786369618080778e-33, 'prefer': 2.6786369618080778e-33, 'join': 2.6786369618080778e-33, 'combine': 2.6786369618080778e-33, 'hit': 2.6786369618080778e-33, 'feature': 2.6786369618080778e-33, 'enable': 2.6786369618080778e-33, 'accomplish': 2.6786369618080778e-33, 'prepare': 2.6786369618080778e-33, 'utilize': 2.6786369618080778e-33, 'sell': 2.6786369618080778e-33, 'squeeze': 2.6786369618080778e-33, 'crush': 2.6786369618080778e-33, 'pound': 2.6786369618080778e-33, 'insert': 2.6786369618080778e-33, 'keep': 2.6786369618080778e-33, 'fight': 2.6786369618080778e-33, 'raise': 2.6786369618080778e-33, 'grab': 2.6786369618080778e-33, 'permit': 2.6786369618080778e-33, 'down': 2.6786369618080778e-33, 'throw': 2.6786369618080778e-33, 'buck': 2.6786369618080778e-33}, 'IA')\n",
      "({'be': 1.0, 'come': 2.9374821117108028e-30, 'make': 1.3336148155022614e-34, 'derive': 6.639677199580735e-36, 'leave': 2.4426007377405277e-36, 'arrive': 8.985825944049381e-37, 'originate': 8.985825944049381e-37, 'run': 6.054601895401186e-39, 'have': 8.194012623990515e-40, 'take': 3.0144087850653746e-40, 'distill': 3.0144087850653746e-40, 'farm': 1.1089390193121365e-40, 'produce': 4.0795586671775603e-41, 'release': 4.0795586671775603e-41, 'go': 4.0795586671775603e-41, 'descend': 1.5007857627073948e-41, 'fall': 1.5007857627073948e-41, 'extract': 5.5210822770285325e-42, 'manufacture': 5.5210822770285325e-42, 'start': 2.031092662734811e-42, 'obtain': 2.031092662734811e-42, 'emerge': 2.031092662734811e-42, 'remove': 7.47197233734299e-43, 'pop': 7.47197233734299e-43, 'use': 2.7487850079102147e-43, 'get': 2.7487850079102147e-43, 'root': 1.0112214926104486e-43, 'move': 1.0112214926104486e-43, 'leak': 1.0112214926104486e-43, 'abscond': 1.0112214926104486e-43, 'hail': 3.720075976020836e-44, 'evolve': 3.720075976020836e-44, 'develop': 3.720075976020836e-44, 'add': 3.720075976020836e-44, 'evaporate': 3.720075976020836e-44, 'elude': 3.720075976020836e-44, 'send': 3.720075976020836e-44, 'br': 3.720075976020836e-44, 'form': 3.720075976020836e-44, 'cook': 3.720075976020836e-44, 'involve': 3.720075976020836e-44, 'include': 3.720075976020836e-44, 'put': 3.720075976020836e-44, 'generate': 3.720075976020836e-44, 'sail': 3.720075976020836e-44, 'drink': 3.720075976020836e-44, 'gain': 1.368539471173853e-44, 'arise': 1.368539471173853e-44, 'flourish': 1.368539471173853e-44, 'grow': 1.368539471173853e-44, 'become': 1.368539471173853e-44, 'collect': 1.368539471173853e-44, 'look': 1.368539471173853e-44, 'found.': 1.368539471173853e-44, 'break': 1.368539471173853e-44, 'taxi': 1.368539471173853e-44, 'date': 1.368539471173853e-44, 'try': 1.368539471173853e-44, 'pour': 1.368539471173853e-44, 'taste': 1.368539471173853e-44, 'refer': 1.368539471173853e-44, 'describe': 1.368539471173853e-44, 'travel': 1.368539471173853e-44, 'feature': 1.368539471173853e-44, 'bail': 1.368539471173853e-44, 'construct': 1.368539471173853e-44, 'emanate': 1.368539471173853e-44, 'gather': 1.368539471173853e-44, 'see': 1.368539471173853e-44, 'give': 1.368539471173853e-44, 'treat': 1.368539471173853e-44, 'receive': 1.368539471173853e-44, 'alarm': 5.0345753587649823e-45, 'distil': 5.0345753587649823e-45, 'yield': 5.0345753587649823e-45, 'drive': 5.0345753587649823e-45, 'bloom': 5.0345753587649823e-45, 'escape': 5.0345753587649823e-45, 'act': 5.0345753587649823e-45, 'delve': 5.0345753587649823e-45, 'tear': 5.0345753587649823e-45, 'identify': 5.0345753587649823e-45, 'deliver': 5.0345753587649823e-45, 'sequence': 5.0345753587649823e-45, 'reach': 5.0345753587649823e-45, 'fire': 5.0345753587649823e-45, 'fund': 5.0345753587649823e-45, 'depart': 5.0345753587649823e-45, 'create': 5.0345753587649823e-45, 'steal.': 5.0345753587649823e-45, 'radiate': 5.0345753587649823e-45, 'achieve': 5.0345753587649823e-45, 'end': 5.0345753587649823e-45, 'win': 5.0345753587649823e-45, 'represent': 5.0345753587649823e-45, 'consider': 5.0345753587649823e-45, 'infiltrate': 5.0345753587649823e-45, 'organize': 5.0345753587649823e-45, 'follow': 5.0345753587649823e-45, 'deice': 5.0345753587649823e-45, 'attribute': 5.0345753587649823e-45, 'juxtapose': 5.0345753587649823e-45, 'strengthen': 5.0345753587649823e-45, 'flee': 5.0345753587649823e-45, 'prove': 5.0345753587649823e-45, 'combine': 5.0345753587649823e-45, 'emerge.': 5.0345753587649823e-45, 'recall': 5.0345753587649823e-45, 'exit': 5.0345753587649823e-45, 'twist': 5.0345753587649823e-45, 'update': 5.0345753587649823e-45, 'graduate': 5.0345753587649823e-45, 'convert': 5.0345753587649823e-45, 'save': 5.0345753587649823e-45, 'mean': 5.0345753587649823e-45, 'set': 5.0345753587649823e-45, 'return': 5.0345753587649823e-45, 'offer': 5.0345753587649823e-45, 'display': 5.0345753587649823e-45, 'supervise': 5.0345753587649823e-45, 'draw': 5.0345753587649823e-45, 'estimate': 5.0345753587649823e-45, 'review': 5.0345753587649823e-45, 'serve': 5.0345753587649823e-45, 'respond': 5.0345753587649823e-45, 'explain': 5.0345753587649823e-45, 'learn': 5.0345753587649823e-45, 'reveal': 5.0345753587649823e-45, 'consult': 5.0345753587649823e-45, 'order': 5.0345753587649823e-45, 'pres': 5.0345753587649823e-45, 'play': 5.0345753587649823e-45, 'climb': 5.0345753587649823e-45, 'base': 5.0345753587649823e-45, 'direct': 5.0345753587649823e-45, 'compose': 5.0345753587649823e-45, 'push': 5.0345753587649823e-45, 'be_given': 5.0345753587649823e-45, 'understand': 5.0345753587649823e-45, 'pull': 5.0345753587649823e-45, 'resign': 5.0345753587649823e-45, 'incorporate': 5.0345753587649823e-45, 'eject': 5.0345753587649823e-45, 'submit': 5.0345753587649823e-45, 'stimulate': 5.0345753587649823e-45, 'copy': 5.0345753587649823e-45, 'be_surprised': 5.0345753587649823e-45, 'solicit': 5.0345753587649823e-45, 'provide': 5.0345753587649823e-45, 'watch': 5.0345753587649823e-45, 'switch': 5.0345753587649823e-45, 'evacuate': 5.0345753587649823e-45, 'find.': 5.0345753587649823e-45, 'i': 5.0345753587649823e-45, 'depend': 5.0345753587649823e-45, 'qualify': 5.0345753587649823e-45, 'say': 5.0345753587649823e-45, 'investigate': 5.0345753587649823e-45, 'sample': 5.0345753587649823e-45, 'catch': 5.0345753587649823e-45, 'throw': 5.0345753587649823e-45, 'group': 5.0345753587649823e-45, 'cure': 5.0345753587649823e-45, 'fuel': 5.0345753587649823e-45, 'popularize': 5.0345753587649823e-45, 'begin': 5.0345753587649823e-45, 'bake': 5.0345753587649823e-45, 'step': 5.0345753587649823e-45, 'notify': 5.0345753587649823e-45, 'star': 5.0345753587649823e-45, 'drown.': 5.0345753587649823e-45, 'mention': 5.0345753587649823e-45, 'reschedule': 5.0345753587649823e-45, 'hand': 5.0345753587649823e-45, 'uprise': 5.0345753587649823e-45, 'lie': 5.0345753587649823e-45, 'kick': 5.0345753587649823e-45, 'leap': 5.0345753587649823e-45, 'sweeten': 5.0345753587649823e-45, 'stem': 5.0345753587649823e-45, 'secure': 5.0345753587649823e-45, 'remain': 5.0345753587649823e-45, 'demolish.': 5.0345753587649823e-45, 'replicate': 5.0345753587649823e-45, 'find': 5.0345753587649823e-45, 'better': 5.0345753587649823e-45, 'lodge': 5.0345753587649823e-45, 'retrieve': 5.0345753587649823e-45, 'share': 5.0345753587649823e-45, 'clog': 5.0345753587649823e-45, 'attract': 5.0345753587649823e-45, 'perform': 5.0345753587649823e-45, 'work': 5.0345753587649823e-45, 'meet': 5.0345753587649823e-45, 'report': 5.0345753587649823e-45, 'steal': 5.0345753587649823e-45, 'harm': 5.0345753587649823e-45, 'recruit': 5.0345753587649823e-45, 'compile': 5.0345753587649823e-45, 'outline': 5.0345753587649823e-45, 'cause': 5.0345753587649823e-45, 'prepare': 5.0345753587649823e-45, 'Reject': 5.0345753587649823e-45}, 'PP')\n",
      "({'cause': 1.0, 'be': 9.92959039626498e-153, 'result': 2.1002384837706373e-171, 'trigger': 1.0456471698030763e-172, 'come': 3.84672096489656e-173, 'produce': 2.5919001981743924e-175, 'generate': 1.2904311236918859e-176, 'lead': 6.424678257926741e-178, 'get': 1.1767208694848799e-179, 'make': 1.1767208694848799e-179, 'radiate': 1.1767208694848799e-179, 'reduce': 1.5925185216216938e-180, 'emit': 5.858548237893603e-181, 'have': 7.928682851306888e-182, 'instigate': 7.928682851306888e-182, 'i': 2.916799416564376e-182, 'arise': 2.916799416564376e-182, 'catch': 2.916799416564376e-182, 'start': 2.916799416564376e-182, 'experience': 2.916799416564376e-182, 'prevent': 2.916799416564376e-182, 'stem': 2.916799416564376e-182, 'create': 2.916799416564376e-182, 'establish': 1.0730305393748917e-182, 'Cause': 1.0730305393748917e-182, 'give': 1.0730305393748917e-182, 'derive': 1.0730305393748917e-182, 'sell': 1.0730305393748917e-182, 'treat': 1.0730305393748917e-182, 'repair': 1.0730305393748917e-182, 'suffer': 1.0730305393748917e-182, 'turn': 3.9474587518512645e-183, 'contribute': 3.9474587518512645e-183, 'originate': 3.9474587518512645e-183, 'relieve': 3.9474587518512645e-183, 'ensue': 3.9474587518512645e-183, 'consider': 3.9474587518512645e-183, 'identify': 3.9474587518512645e-183, 'develop': 3.9474587518512645e-183, 'hit': 3.9474587518512645e-183, 'face': 3.9474587518512645e-183, 'reveal': 3.9474587518512645e-183, 'feel': 3.9474587518512645e-183, 'contain': 1.4521889196783625e-183, 'affect': 1.4521889196783625e-183, 'regard': 1.4521889196783625e-183, 'responsible': 1.4521889196783625e-183, 'spill': 1.4521889196783625e-183, 'scream': 1.4521889196783625e-183, 'resolve': 1.4521889196783625e-183, 'destroy': 1.4521889196783625e-183, 'describe': 1.4521889196783625e-183, 'alleviate': 1.4521889196783625e-183, 'gain': 1.4521889196783625e-183, 'root': 1.4521889196783625e-183, 'put': 1.4521889196783625e-183, 'set': 1.4521889196783625e-183, 'show': 1.4521889196783625e-183, 'help': 1.4521889196783625e-183, 'increase': 1.4521889196783625e-183, 'withdraw': 1.4521889196783625e-183, 'fear': 1.4521889196783625e-183, 'recognize': 1.4521889196783625e-183, 'follow': 1.4521889196783625e-183, 'br': 1.4521889196783625e-183, 'reach': 1.4521889196783625e-183, 'provide': 1.4521889196783625e-183, 'minimize': 1.4521889196783625e-183, 'find': 1.4521889196783625e-183, 'threaten': 1.4521889196783625e-183, 'trigger.': 5.342304482466365e-184, 'sw': 5.342304482466365e-184, 'close': 5.342304482466365e-184, 'preclude': 5.342304482466365e-184, 'release': 5.342304482466365e-184, 'mark': 5.342304482466365e-184, 'cau': 5.342304482466365e-184, 'offer': 5.342304482466365e-184, 'enter': 5.342304482466365e-184, 'convert': 5.342304482466365e-184, 'acknowledge': 5.342304482466365e-184, 'push': 5.342304482466365e-184, 'avoid': 5.342304482466365e-184, 'transmit': 5.342304482466365e-184, 'cite': 5.342304482466365e-184, 'source': 5.342304482466365e-184, 'strike': 5.342304482466365e-184, 'take': 5.342304482466365e-184, 'burst': 5.342304482466365e-184, 'achieve': 5.342304482466365e-184, 'correct': 5.342304482466365e-184, 'account': 5.342304482466365e-184, 'inspect': 5.342304482466365e-184, 'entitle': 5.342304482466365e-184, 'relate': 5.342304482466365e-184, 'output': 5.342304482466365e-184, 'attribut': 5.342304482466365e-184, 'ensure': 5.342304482466365e-184, 'move': 5.342304482466365e-184, 'play': 5.342304482466365e-184, 'become': 5.342304482466365e-184, 'fall': 5.342304482466365e-184, 'combat': 5.342304482466365e-184, 'suffocation': 5.342304482466365e-184, 'cause\\n': 5.342304482466365e-184, 'stimulate': 5.342304482466365e-184, 'transact': 5.342304482466365e-184, 'visit': 5.342304482466365e-184, 'depend': 5.342304482466365e-184, 'reverse': 5.342304482466365e-184, 'run': 5.342304482466365e-184, 'direct': 5.342304482466365e-184, 'make.': 5.342304482466365e-184, 'tug': 5.342304482466365e-184, 'detach': 5.342304482466365e-184, 'eat': 5.342304482466365e-184, 'conceptualize': 5.342304482466365e-184, 'neutralize': 5.342304482466365e-184, 'arrive': 5.342304482466365e-184, 'proclaim': 5.342304482466365e-184, 'initiate': 5.342304482466365e-184, 'read': 5.342304482466365e-184, 'survive': 5.342304482466365e-184, 'shape': 5.342304482466365e-184, 'asses': 5.342304482466365e-184, 'unite': 5.342304482466365e-184, 'prepare': 5.342304482466365e-184, 'know': 5.342304482466365e-184, 'translate': 5.342304482466365e-184, 'craft': 5.342304482466365e-184, 'deprive': 5.342304482466365e-184, 'retain': 5.342304482466365e-184, 'mov': 5.342304482466365e-184, 'decide': 5.342304482466365e-184, 'support': 5.342304482466365e-184, 'eliminate': 5.342304482466365e-184, 'depart': 5.342304482466365e-184, 'cough': 5.342304482466365e-184, 'boost': 5.342304482466365e-184, 'grow': 5.342304482466365e-184, 'weaken': 5.342304482466365e-184, 'reside': 5.342304482466365e-184, 'consist': 5.342304482466365e-184, 'batter': 5.342304482466365e-184, 'send': 5.342304482466365e-184, 'expose': 5.342304482466365e-184, 'succumb': 5.342304482466365e-184, 'share': 5.342304482466365e-184, 'burn': 5.342304482466365e-184, 'overstate': 5.342304482466365e-184, 'accommodate': 5.342304482466365e-184, 'contract': 5.342304482466365e-184, 'check': 5.342304482466365e-184, 'inundate': 5.342304482466365e-184, 'donate': 5.342304482466365e-184, 'end': 5.342304482466365e-184, 'Place': 5.342304482466365e-184, 'offset': 5.342304482466365e-184, 'spread': 5.342304482466365e-184, 'fill': 5.342304482466365e-184, 'subject': 5.342304482466365e-184, 'intubation': 5.342304482466365e-184, 'receive': 5.342304482466365e-184, 'essential': 5.342304482466365e-184, 'mitigate': 5.342304482466365e-184, 'choke': 5.342304482466365e-184, 'await': 5.342304482466365e-184, 'quench': 5.342304482466365e-184, 'change': 5.342304482466365e-184, 'improve': 5.342304482466365e-184, 'see': 5.342304482466365e-184, 'teach': 5.342304482466365e-184, 'enhance': 5.342304482466365e-184, 'sale_of': 5.342304482466365e-184, 'resist': 5.342304482466365e-184, 'drink': 5.342304482466365e-184, 'smile': 5.342304482466365e-184, 'demonstrate': 5.342304482466365e-184, 'happen': 5.342304482466365e-184, 'occur': 5.342304482466365e-184, 'bombard': 5.342304482466365e-184, 'include': 5.342304482466365e-184, 'allow': 5.342304482466365e-184, 'delay': 5.342304482466365e-184, 'sleep': 5.342304482466365e-184, 'respond': 5.342304482466365e-184, 'block': 5.342304482466365e-184, 'brace': 5.342304482466365e-184, 'producean': 5.342304482466365e-184, 'after': 5.342304482466365e-184, 'obtain': 5.342304482466365e-184, 'examine': 5.342304482466365e-184, 'mediate': 5.342304482466365e-184, 'emanate': 5.342304482466365e-184, 'consumption': 5.342304482466365e-184, 'beget': 5.342304482466365e-184, 'utter': 5.342304482466365e-184, 'connect': 5.342304482466365e-184, 'counteract': 5.342304482466365e-184}, 'CE')\n",
      "({'be': 1.0, 'have': 1.9287498479639178e-22, 'find': 2.1705220113036395e-29, 'consist': 2.1705220113036395e-29, 'join': 1.0806392777072785e-30, 'bring': 1.0806392777072785e-30, 'come': 1.0806392777072785e-30, 'contain': 3.975449735908647e-31, 'become': 1.462486227251231e-31, 'make': 1.462486227251231e-31, 'take': 1.462486227251231e-31, 'form': 1.462486227251231e-31, 'establish': 1.462486227251231e-31, 'attack': 5.380186160021138e-32, 'build': 5.380186160021138e-32, 'stand': 5.380186160021138e-32, 'live': 5.380186160021138e-32, 'see': 5.380186160021138e-32, 'develop': 5.380186160021138e-32, 'comprise': 1.9792598779469045e-32, 'drive': 1.9792598779469045e-32, 'gather': 1.9792598779469045e-32, 'compose': 1.9792598779469045e-32, 'start': 1.9792598779469045e-32, 'produce': 1.9792598779469045e-32, 'spot': 1.9792598779469045e-32, 'send': 1.9792598779469045e-32, 'create': 1.9792598779469045e-32, 'fly': 1.9792598779469045e-32, 'include': 7.281290178321643e-33, 'call': 7.281290178321643e-33, 'serve': 7.281290178321643e-33, 'keep': 7.281290178321643e-33, 'get': 7.281290178321643e-33, 'study': 7.281290178321643e-33, 'leave': 7.281290178321643e-33, 'arrive': 7.281290178321643e-33, 'fall': 7.281290178321643e-33, 'select': 7.281290178321643e-33, 'organize': 7.281290178321643e-33, 'maintain': 7.281290178321643e-33, 'watch': 7.281290178321643e-33, 'lead': 7.281290178321643e-33, 'provide': 7.281290178321643e-33, 'show': 7.281290178321643e-33, 'perform': 7.281290178321643e-33, 'support': 7.281290178321643e-33, 'work': 7.281290178321643e-33, 'collect': 2.6786369618080778e-33, 'pull': 2.6786369618080778e-33, 'move': 2.6786369618080778e-33, 'pas': 2.6786369618080778e-33, 'swim': 2.6786369618080778e-33, 'observe': 2.6786369618080778e-33, 'reflect': 2.6786369618080778e-33, 'investigate': 2.6786369618080778e-33, 'increase': 2.6786369618080778e-33, 'migrate': 2.6786369618080778e-33, 'splash': 2.6786369618080778e-33, 'prove': 2.6786369618080778e-33, 'dwell': 2.6786369618080778e-33, 'recapture': 2.6786369618080778e-33, 'employ': 2.6786369618080778e-33, 'turn': 2.6786369618080778e-33, 'use': 2.6786369618080778e-33, 'play': 2.6786369618080778e-33, 'pick': 2.6786369618080778e-33, 'purchase': 2.6786369618080778e-33, 'carry': 2.6786369618080778e-33, 'return': 2.6786369618080778e-33, 'meet': 2.6786369618080778e-33, 'describe': 2.6786369618080778e-33, 'sign': 2.6786369618080778e-33, 'expose': 2.6786369618080778e-33, 'sell': 2.6786369618080778e-33, 'i': 2.6786369618080778e-33, 'represent': 2.6786369618080778e-33, 'fee': 2.6786369618080778e-33, 'involve': 2.6786369618080778e-33, 'issue': 2.6786369618080778e-33, 'set': 2.6786369618080778e-33, 'follow': 2.6786369618080778e-33, 'harbor': 2.6786369618080778e-33, 'announce': 2.6786369618080778e-33, 'descend': 2.6786369618080778e-33, 'face': 2.6786369618080778e-33, 'train': 2.6786369618080778e-33, 'associate': 2.6786369618080778e-33, 'decide': 2.6786369618080778e-33, 'mobilize': 2.6786369618080778e-33, 'fight': 2.6786369618080778e-33, 'constitute': 2.6786369618080778e-33, 'witnes': 2.6786369618080778e-33, 'hire': 2.6786369618080778e-33, 'surround': 2.6786369618080778e-33, 'drift': 2.6786369618080778e-33, 'handle': 2.6786369618080778e-33, 'shake': 2.6786369618080778e-33, 'enter': 2.6786369618080778e-33, 'look': 2.6786369618080778e-33, 'back': 9.854154686111257e-34, 'amount': 9.854154686111257e-34, 'lie': 9.854154686111257e-34, 'degenerate': 9.854154686111257e-34, 'arm': 9.854154686111257e-34, 'complement': 9.854154686111257e-34, 'challenge': 9.854154686111257e-34, 'Pas': 9.854154686111257e-34, 'offer': 9.854154686111257e-34, 'direct': 9.854154686111257e-34, 'bear': 9.854154686111257e-34, 'visit': 9.854154686111257e-34, 'lure': 9.854154686111257e-34, 'tremble': 9.854154686111257e-34, 'exposure': 9.854154686111257e-34, 'raise': 9.854154686111257e-34, 'organise': 9.854154686111257e-34, 'board': 9.854154686111257e-34, 'attend': 9.854154686111257e-34, 'perch': 9.854154686111257e-34, 'hear': 9.854154686111257e-34, 'marvell': 9.854154686111257e-34, 'impres': 9.854154686111257e-34, 'assemble': 9.854154686111257e-34, 'lay': 9.854154686111257e-34, 'continue': 9.854154686111257e-34, 'plague': 9.854154686111257e-34, 'enroll': 9.854154686111257e-34, 'cause': 9.854154686111257e-34, 'deploy': 9.854154686111257e-34, 'cros': 9.854154686111257e-34, 'pay': 9.854154686111257e-34, 'manage': 9.854154686111257e-34, 'encounter': 9.854154686111257e-34, 'skulk': 9.854154686111257e-34, 'identify': 9.854154686111257e-34, 'member': 9.854154686111257e-34, 'feel': 9.854154686111257e-34, 'storm': 9.854154686111257e-34, 'abandon': 9.854154686111257e-34, 'devise': 9.854154686111257e-34, 'wait': 9.854154686111257e-34, 'serenade': 9.854154686111257e-34, 'dash': 9.854154686111257e-34, 'secure': 9.854154686111257e-34, 'fill': 9.854154686111257e-34, 'cl': 9.854154686111257e-34, 'take.': 9.854154686111257e-34, 'float': 9.854154686111257e-34, 'bask': 9.854154686111257e-34, 'choose': 9.854154686111257e-34, 'plow': 9.854154686111257e-34, 'experience': 9.854154686111257e-34, 'chair': 9.854154686111257e-34, 'stack': 9.854154686111257e-34, 'arrest': 9.854154686111257e-34, 'receive': 9.854154686111257e-34, 'reconstitute': 9.854154686111257e-34, 'depict': 9.854154686111257e-34, 'connect': 9.854154686111257e-34, 'initiate': 9.854154686111257e-34, 'say': 9.854154686111257e-34, 'draw': 9.854154686111257e-34, 'coo': 9.854154686111257e-34, 'focu': 9.854154686111257e-34, 'reduce': 9.854154686111257e-34, 'presid': 9.854154686111257e-34, 'loan': 9.854154686111257e-34, 'scavenge': 9.854154686111257e-34, 'run': 9.854154686111257e-34, 'bellow': 9.854154686111257e-34, 'attract': 9.854154686111257e-34, 'embrace': 9.854154686111257e-34, 'feast': 9.854154686111257e-34, 'glid': 9.854154686111257e-34, 'Interact': 9.854154686111257e-34, 'burst': 9.854154686111257e-34, 'Overrun': 9.854154686111257e-34, 'resign': 9.854154686111257e-34, 'vote': 9.854154686111257e-34, 'drop': 9.854154686111257e-34, 'exist': 9.854154686111257e-34, 'trot': 9.854154686111257e-34, 'sire': 9.854154686111257e-34, 'entertain': 9.854154686111257e-34, 'seize': 9.854154686111257e-34, 'share': 9.854154686111257e-34, 'suspend': 9.854154686111257e-34, 'change': 9.854154686111257e-34, 'examine': 9.854154686111257e-34, 'forge': 9.854154686111257e-34, 'close': 9.854154686111257e-34, 'appear': 9.854154686111257e-34, 'contend': 9.854154686111257e-34, 'infest': 9.854154686111257e-34, 'reteam': 9.854154686111257e-34, 'struggle': 9.854154686111257e-34, 'defeat': 9.854154686111257e-34, 'conquer': 9.854154686111257e-34, 'invest': 9.854154686111257e-34, 'rent': 9.854154686111257e-34, 'cadge': 9.854154686111257e-34, 'hang': 9.854154686111257e-34, 'acquir': 9.854154686111257e-34, 'obtain': 9.854154686111257e-34, 'assert': 9.854154686111257e-34, 'point': 9.854154686111257e-34, 'hunt': 9.854154686111257e-34, 'rope': 9.854154686111257e-34, 'integrate': 9.854154686111257e-34, 'ease': 9.854154686111257e-34, 'place': 9.854154686111257e-34, 'discus': 9.854154686111257e-34, 'emerge': 9.854154686111257e-34, 'rail': 9.854154686111257e-34, 'remind': 9.854154686111257e-34, 'fit': 9.854154686111257e-34, 'teach': 9.854154686111257e-34, 'depend': 9.854154686111257e-34, 'replace': 9.854154686111257e-34, 'bind': 9.854154686111257e-34, 'notice': 9.854154686111257e-34, 'deny': 9.854154686111257e-34, 'cover': 9.854154686111257e-34, 'jostl': 9.854154686111257e-34, 'charm': 9.854154686111257e-34, 'arrange': 9.854154686111257e-34, 'skip': 9.854154686111257e-34, 'comment': 9.854154686111257e-34, 'crouch': 9.854154686111257e-34, 'sound': 9.854154686111257e-34, 'abduct': 9.854154686111257e-34, 'aim': 9.854154686111257e-34, 'Br': 9.854154686111257e-34, 'accompany': 9.854154686111257e-34, 'assault': 9.854154686111257e-34, 'cast': 9.854154686111257e-34, 'destroy': 9.854154686111257e-34, 'ingest': 9.854154686111257e-34, 'begin': 9.854154686111257e-34, 'rely': 9.854154686111257e-34, 'stare': 9.854154686111257e-34, 'grant': 9.854154686111257e-34, 'speak': 9.854154686111257e-34, 'appoint': 9.854154686111257e-34, 'party': 9.854154686111257e-34, 'portray': 9.854154686111257e-34, 'stay': 9.854154686111257e-34, 'devote': 9.854154686111257e-34, 'order': 9.854154686111257e-34, 'soar': 9.854154686111257e-34, 'purr': 9.854154686111257e-34, 'auction': 9.854154686111257e-34, 'expect': 9.854154686111257e-34, 'sacrifice': 9.854154686111257e-34, 'result': 9.854154686111257e-34, 'battle': 9.854154686111257e-34, 'belong': 9.854154686111257e-34, 'advocate': 9.854154686111257e-34, 'contribute': 9.854154686111257e-34, 'pit': 9.854154686111257e-34, 'betray': 9.854154686111257e-34, 'catch': 9.854154686111257e-34, 'recall': 9.854154686111257e-34, 'tumble': 9.854154686111257e-34, 'tease': 9.854154686111257e-34, 'yell': 9.854154686111257e-34, 'tell': 9.854154686111257e-34, 'march': 9.854154686111257e-34, 'promote': 9.854154686111257e-34, 'contract': 9.854154686111257e-34, 'feature': 9.854154686111257e-34, 'step': 9.854154686111257e-34, 'brave': 9.854154686111257e-34, 'resemble': 9.854154686111257e-34, 'reject': 9.854154686111257e-34, 'nest': 9.854154686111257e-34, 'shift': 9.854154686111257e-34, 'deal': 9.854154686111257e-34, 'stumble': 9.854154686111257e-34, 'imagine': 9.854154686111257e-34, 'name': 9.854154686111257e-34, 'scurry': 9.854154686111257e-34, 'convene': 9.854154686111257e-34, 'generate': 9.854154686111257e-34, 'motivat': 9.854154686111257e-34, 'adapt': 9.854154686111257e-34, 'try': 9.854154686111257e-34, 'undulate': 9.854154686111257e-34, 'conceptualize': 9.854154686111257e-34, 'discover': 9.854154686111257e-34, 'swarm': 9.854154686111257e-34, 'read': 9.854154686111257e-34, 'season': 9.854154686111257e-34, 'appeal': 9.854154686111257e-34, 'realize': 9.854154686111257e-34, 'drown': 9.854154686111257e-34, 'unfold': 9.854154686111257e-34, 'populate': 9.854154686111257e-34, 'trigger': 9.854154686111257e-34, 'rescue': 9.854154686111257e-34, 'sight': 9.854154686111257e-34, 'mark': 9.854154686111257e-34, 'supply': 9.854154686111257e-34, 'confirm': 9.854154686111257e-34, 'walk': 9.854154686111257e-34, 'undertake': 9.854154686111257e-34, 'detect': 9.854154686111257e-34, 'survive': 9.854154686111257e-34, 'latch': 9.854154686111257e-34, 'traverse': 9.854154686111257e-34, 'convert': 9.854154686111257e-34, 'flee': 9.854154686111257e-34, 'satisfy': 9.854154686111257e-34, 'gallop': 9.854154686111257e-34, 'erupt': 9.854154686111257e-34, 'utilization': 9.854154686111257e-34, 'advance': 9.854154686111257e-34, 'locate': 9.854154686111257e-34, 'sail': 9.854154686111257e-34, 'Picture': 9.854154686111257e-34, 'exhume': 9.854154686111257e-34, 'publish': 9.854154686111257e-34, 'approve': 9.854154686111257e-34, 'compete': 9.854154686111257e-34, 'predicate': 9.854154686111257e-34, 'evaluate': 9.854154686111257e-34, 'trim': 9.854154686111257e-34, 'let': 9.854154686111257e-34, 'launch': 9.854154686111257e-34, 'break': 9.854154686111257e-34, 'scold': 9.854154686111257e-34, 'indicate': 9.854154686111257e-34, 'explain': 9.854154686111257e-34, 'whirl': 9.854154686111257e-34, 'fortify': 9.854154686111257e-34, 'bounce': 9.854154686111257e-34, 'gamble': 9.854154686111257e-34, 'chase': 9.854154686111257e-34, 'prepare': 9.854154686111257e-34, 'evince': 9.854154686111257e-34, 'cal': 9.854154686111257e-34, 'breathe': 9.854154686111257e-34}, 'MC')\n",
      "({'be': 0.9999999992417439, 'contain': 7.582560422162384e-10, 'find': 1.7587922010906967e-25, 'store': 4.35960999675738e-28, 'enclose': 7.984904239632376e-30, 'discover': 5.38018615594158e-32, 'lock': 7.28129017280056e-33, 'keep': 2.678636959776985e-33, 'hide': 9.854154678639285e-34, 'place': 3.625140916394774e-34, 'carry': 3.625140916394774e-34, 'put': 3.625140916394774e-34, 'hold': 3.625140916394774e-34, 'wa': 1.3336148144910399e-34, 'have': 1.8048513864768756e-35, 'br': 6.639677194546159e-36, 'include': 6.639677194546159e-36, 'seal': 2.4426007358884107e-36, 'fill': 2.4426007358884107e-36, 'make': 2.4426007358884107e-36, 'serve': 8.985825937235824e-37, 'give': 8.985825937235824e-37, 'sit': 8.985825937235824e-37, 'cover': 8.985825937235824e-37, 'open': 8.985825937235824e-37, 'see': 8.985825937235824e-37, 'eat': 3.3057006242541667e-37, 'i': 3.3057006242541667e-37, 'wrap': 3.3057006242541667e-37, 'send': 3.3057006242541667e-37, 'fit': 3.3057006242541667e-37, 'measure': 3.3057006242541667e-37, 'pull': 3.3057006242541667e-37, 'tuck': 3.3057006242541667e-37, 'use': 3.3057006242541667e-37, 'determine': 3.3057006242541667e-37, 'hav': 3.3057006242541667e-37, 'container': 3.3057006242541667e-37, 'explode': 3.3057006242541667e-37, '\"contains\"': 3.3057006242541667e-37, 'leave': 3.3057006242541667e-37, 'remove': 3.3057006242541667e-37, 'Wa': 3.3057006242541667e-37, 'go': 1.2160992983307109e-37, 'date': 1.2160992983307109e-37, 'preserve': 1.2160992983307109e-37, 'lose': 1.2160992983307109e-37, 'strike': 1.2160992983307109e-37, 'coil': 1.2160992983307109e-37, 'stor': 1.2160992983307109e-37, 'scratch': 1.2160992983307109e-37, 'secure': 1.2160992983307109e-37, 'arrive': 1.2160992983307109e-37, 'attach': 1.2160992983307109e-37, 'introduce': 1.2160992983307109e-37, 'symbolise': 1.2160992983307109e-37, 'hang': 1.2160992983307109e-37, 'receive': 1.2160992983307109e-37, 'throw': 1.2160992983307109e-37, 'weigh': 1.2160992983307109e-37, 'kill': 1.2160992983307109e-37, 'label': 1.2160992983307109e-37, 'fall': 1.2160992983307109e-37, 'pour': 1.2160992983307109e-37, 'note': 1.2160992983307109e-37, 'connect': 1.2160992983307109e-37, 'cost': 1.2160992983307109e-37, '\"shows\"': 1.2160992983307109e-37, 'print': 1.2160992983307109e-37, 'unpack': 1.2160992983307109e-37, 'stand': 1.2160992983307109e-37, 'take': 1.2160992983307109e-37, 'full': 1.2160992983307109e-37, 'own': 1.2160992983307109e-37, 'accept': 1.2160992983307109e-37, 'rattle': 1.2160992983307109e-37, 'drink': 1.2160992983307109e-37, 'ensure': 1.2160992983307109e-37, 'cease': 1.2160992983307109e-37, 'ha': 1.2160992983307109e-37, 'stir': 1.2160992983307109e-37, 'ensconce': 1.2160992983307109e-37, 'dissolve': 1.2160992983307109e-37, 'generate': 1.2160992983307109e-37, 'inventory': 1.2160992983307109e-37, 'house': 1.2160992983307109e-37, 'examine': 1.2160992983307109e-37, 'tak': 1.2160992983307109e-37, 'inside': 1.2160992983307109e-37, 'Find': 1.2160992983307109e-37, '\"inside\"': 1.2160992983307109e-37, 'bury': 1.2160992983307109e-37, 'Remember': 1.2160992983307109e-37, 'show': 1.2160992983307109e-37, 'pas': 1.2160992983307109e-37, 'auction.': 1.2160992983307109e-37, 'turn': 1.2160992983307109e-37, 'become': 1.2160992983307109e-37, 'See': 1.2160992983307109e-37, 'sleep': 1.2160992983307109e-37, 'stuff': 1.2160992983307109e-37, 'snatch': 1.2160992983307109e-37, 'arrange': 1.2160992983307109e-37, 'offer': 1.2160992983307109e-37, 'cram': 1.2160992983307109e-37, 'retriev': 1.2160992983307109e-37, 'ship': 1.2160992983307109e-37, 'try': 1.2160992983307109e-37, 'impart': 1.2160992983307109e-37, 'bind': 1.2160992983307109e-37, 'protect': 1.2160992983307109e-37, 'conceal': 1.2160992983307109e-37, 'fly': 1.2160992983307109e-37, 'get': 1.2160992983307109e-37, 'deposit': 1.2160992983307109e-37, 'lock.': 1.2160992983307109e-37, 'play': 1.2160992983307109e-37, 'escape': 1.2160992983307109e-37, 'display': 1.2160992983307109e-37, 'come': 1.2160992983307109e-37, 'enter': 1.2160992983307109e-37, 'sort': 1.2160992983307109e-37, 'seize': 1.2160992983307109e-37, 'check': 1.2160992983307109e-37, 'grow': 1.2160992983307109e-37, 'mistake': 1.2160992983307109e-37, 'Notice': 1.2160992983307109e-37, 'surround': 1.2160992983307109e-37}, 'EO')\n",
      "({'be': 0.9999999999991966, 'describe': 6.914400106934648e-13, 'make': 9.357622968832657e-14, 'discus': 1.2664165549084002e-14, 'reflect': 4.6588861450996546e-15, 'explain': 6.305116760141924e-16, 'relate': 2.3195228302417064e-16, 'have': 3.139132792045508e-17, 'depict': 3.139132792045508e-17, 'contain': 3.139132792045508e-17, 'report': 3.139132792045508e-17, 'give': 1.154822417300651e-17, 'provide': 1.154822417300651e-17, 'explore': 1.154822417300651e-17, 'investigate': 4.248354255288176e-18, 'point': 4.248354255288176e-18, 'examine': 4.248354255288176e-18, 'illustrate': 1.5628821893337333e-18, 'define': 1.5628821893337333e-18, 'inform': 1.5628821893337333e-18, 'document': 1.5628821893337333e-18, 'advertise': 5.749522264288941e-19, 'center': 5.749522264288941e-19, 'declare': 5.749522264288941e-19, 'concern': 5.749522264288941e-19, 'outline': 5.749522264288941e-19, 'focu': 5.749522264288941e-19, 'become': 5.749522264288941e-19, 'deal': 5.749522264288941e-19, 'talk': 2.1151310375893813e-19, 'form': 2.1151310375893813e-19, 'publish': 2.1151310375893813e-19, 'supply': 2.1151310375893813e-19, 'cover': 2.1151310375893813e-19, 'summarise': 2.1151310375893813e-19, 'consider': 2.1151310375893813e-19, 'set': 2.1151310375893813e-19, 'narrate': 7.781132241127546e-20, 'offer': 7.781132241127546e-20, 'devote': 7.781132241127546e-20, 'apply': 7.781132241127546e-20, 'show': 7.781132241127546e-20, 'study': 7.781132241127546e-20, 'criticize': 7.781132241127546e-20, 'run': 7.781132241127546e-20, 'tell': 7.781132241127546e-20, 'speak': 7.781132241127546e-20, 'feature': 7.781132241127546e-20, 'develop': 7.781132241127546e-20, 'comment': 7.781132241127546e-20, 'detail': 7.781132241127546e-20, 'receive': 7.781132241127546e-20, 'introduce': 7.781132241127546e-20, 'come': 7.781132241127546e-20, 'tackle': 7.781132241127546e-20, 'highlight': 7.781132241127546e-20, 'appear': 7.781132241127546e-20, 'mention': 2.862518580547094e-20, 'analyse': 2.862518580547094e-20, 'prepare': 2.862518580547094e-20, 'include': 2.862518580547094e-20, 'find': 2.862518580547094e-20, 'release': 2.862518580547094e-20, 'assert': 2.862518580547094e-20, 'swirl': 2.862518580547094e-20, 'learn': 2.862518580547094e-20, 'create': 2.862518580547094e-20, 'adopt': 2.862518580547094e-20, 'addres': 2.862518580547094e-20, 'involve': 2.862518580547094e-20, 'compare': 2.862518580547094e-20, 'reveal': 2.862518580547094e-20, 'survey': 2.862518580547094e-20, 'state': 2.862518580547094e-20, 'remind': 1.0530617357545351e-20, 'convene': 1.0530617357545351e-20, 'analyze': 1.0530617357545351e-20, 'obtain': 1.0530617357545351e-20, 'promote': 1.0530617357545351e-20, 'raise': 1.0530617357545351e-20, 'produce': 1.0530617357545351e-20, 'hold': 1.0530617357545351e-20, 'portray': 1.0530617357545351e-20, 'use': 1.0530617357545351e-20, 'teach': 1.0530617357545351e-20, 'send': 1.0530617357545351e-20, 'insist': 1.0530617357545351e-20, 'criticise': 1.0530617357545351e-20, 'take': 1.0530617357545351e-20, 'go': 1.0530617357545351e-20, 'review': 1.0530617357545351e-20, 'base': 1.0530617357545351e-20, 'chart': 1.0530617357545351e-20, 'conduct': 1.0530617357545351e-20, 'present': 1.0530617357545351e-20, 'pertain': 1.0530617357545351e-20, 'encourage': 1.0530617357545351e-20, 'establish': 1.0530617357545351e-20, 'govern': 1.0530617357545351e-20, 'accept': 1.0530617357545351e-20, 'expres': 1.0530617357545351e-20, 'write': 1.0530617357545351e-20, 'allege': 1.0530617357545351e-20, 'stimulate': 1.0530617357545351e-20, 'br': 1.0530617357545351e-20, 'conclude': 1.0530617357545351e-20, 'issue': 3.8739976286840745e-21, 'construct': 3.8739976286840745e-21, 'incorporate': 3.8739976286840745e-21, 'launch': 3.8739976286840745e-21, 'respond': 3.8739976286840745e-21, 'Be': 3.8739976286840745e-21, 'inquire': 3.8739976286840745e-21, 'comprise': 3.8739976286840745e-21, 'represent': 3.8739976286840745e-21, 'emphasise': 3.8739976286840745e-21, 'serve': 3.8739976286840745e-21, 'rotate': 3.8739976286840745e-21, 'achieve': 3.8739976286840745e-21, 'differ': 3.8739976286840745e-21, 'provoke': 3.8739976286840745e-21, 'submit': 3.8739976286840745e-21, 'engage': 3.8739976286840745e-21, 'begin': 3.8739976286840745e-21, 'indicate': 3.8739976286840745e-21, 'witnes': 3.8739976286840745e-21, 'pas': 3.8739976286840745e-21, 'recount': 3.8739976286840745e-21, 'attribute': 3.8739976286840745e-21, 'summarize': 3.8739976286840745e-21, 'furnish': 3.8739976286840745e-21, 'dominate': 3.8739976286840745e-21, 'surround': 3.8739976286840745e-21, 'enter': 3.8739976286840745e-21, 'keep': 3.8739976286840745e-21, 'solve': 3.8739976286840745e-21, 'approache': 3.8739976286840745e-21, 'discusse': 3.8739976286840745e-21, 'trace': 3.8739976286840745e-21, 'approach': 3.8739976286840745e-21, 'direct': 3.8739976286840745e-21, 'suspend': 3.8739976286840745e-21, 'retrieve': 3.8739976286840745e-21, 'drive': 3.8739976286840745e-21, 'consider.': 3.8739976286840745e-21, 'reach': 3.8739976286840745e-21, 'emphasize': 3.8739976286840745e-21, 'add': 3.8739976286840745e-21, 'evolve': 3.8739976286840745e-21, 'erupt': 3.8739976286840745e-21, 'meet': 3.8739976286840745e-21, 'ad': 3.8739976286840745e-21, 'paint': 3.8739976286840745e-21, 'capture': 3.8739976286840745e-21, 'facilitate': 3.8739976286840745e-21, 'say': 3.8739976286840745e-21, 'occur': 3.8739976286840745e-21, 'pronounce': 3.8739976286840745e-21, 'demand': 3.8739976286840745e-21, 'design.': 3.8739976286840745e-21, 'need': 3.8739976286840745e-21, 'tabulate': 3.8739976286840745e-21, 'propose': 3.8739976286840745e-21, 'specialize': 3.8739976286840745e-21, 'answer': 3.8739976286840745e-21, 'move': 3.8739976286840745e-21, 'get': 3.8739976286840745e-21, 'sw': 3.8739976286840745e-21, 'refer': 3.8739976286840745e-21, 'open': 3.8739976286840745e-21, 'limit': 3.8739976286840745e-21, 'look': 3.8739976286840745e-21, 'make.': 3.8739976286840745e-21, 'dedicate': 3.8739976286840745e-21, 'explore.': 3.8739976286840745e-21, 'organise': 3.8739976286840745e-21, 'regulate': 3.8739976286840745e-21, 'piece': 3.8739976286840745e-21, 'position': 3.8739976286840745e-21, 'connect': 3.8739976286840745e-21, 'characterize': 3.8739976286840745e-21, 'increase': 3.8739976286840745e-21}, 'CT')\n",
      "({'place': 0.9525685137935131, 'put': 0.047425593721312186, 'move': 5.852783229841597e-06, 'pour': 3.9435743199808935e-08, 'send': 2.657159475498577e-10, 'migrate': 2.4230158053094497e-13, 'release': 2.4230158053094497e-13, 'add': 6.0060557015077885e-16, 'spread': 2.2095044151152405e-16, 'deliver': 8.128312494984291e-17, 'insert': 8.128312494984291e-17, 'arrive': 8.128312494984291e-17, 'drop': 2.990239058321673e-17, 'drain': 4.046848499031456e-18, 'import': 1.4887523643291822e-18, 'throw': 1.4887523643291822e-18, 'blow': 5.476813878320831e-19, 'invest': 5.476813878320831e-19, 'travel': 2.0148072289566669e-19, 'donate': 2.0148072289566669e-19, 'transport': 7.412061574567609e-20, 'take': 7.412061574567609e-20, 'stuff': 2.7267450699802525e-20, 'inject': 2.7267450699802525e-20, 'pas': 2.7267450699802525e-20, 'export': 1.0031134525613207e-20, 'leak': 1.0031134525613207e-20, 'enter': 1.0031134525613207e-20, 'carry': 3.6902481635981475e-21, 'push': 3.6902481635981475e-21, 'run': 3.6902481635981475e-21, 'flow': 3.6902481635981475e-21, 'br': 3.6902481635981475e-21, 'go': 1.3575664322084281e-21, 'dump': 1.3575664322084281e-21, 'drag': 1.3575664322084281e-21, 'land': 1.3575664322084281e-21, 'journey': 1.3575664322084281e-21, 'implant': 1.3575664322084281e-21, 'fetch': 4.994207804339452e-22, 'give': 4.994207804339452e-22, 'ship': 4.994207804339452e-22, 'hand': 4.994207804339452e-22, 'fly': 1.8372663761544536e-22, 'remove': 1.8372663761544536e-22, 'fall': 1.8372663761544536e-22, 'reschedule': 1.8372663761544536e-22, 'misplace': 1.8372663761544536e-22, 'sink': 6.758925277427813e-23, 'approach': 6.758925277427813e-23, 'postpone': 2.4864696539796798e-23, 'pack': 2.4864696539796798e-23, 'award': 2.4864696539796798e-23, 'be': 2.4864696539796798e-23, 'enclose': 2.4864696539796798e-23, 'seal': 9.14721066795794e-24, 'post': 9.14721066795794e-24, 'store': 9.14721066795794e-24, 'Pour': 9.14721066795794e-24, 'bestow': 9.14721066795794e-24, 'lock': 3.365070748805823e-24, 'start': 3.365070748805823e-24, 'come': 3.365070748805823e-24, 'collect': 3.365070748805823e-24, 'install': 3.365070748805823e-24, 'load': 1.2379403465730533e-24, 'incorporate': 1.2379403465730533e-24, 'slip': 1.2379403465730533e-24, 'inherit': 1.2379403465730533e-24, 'transfer': 1.2379403465730533e-24, 'deposit': 1.2379403465730533e-24, 'Move': 1.2379403465730533e-24, 'sail': 1.2379403465730533e-24, 'retract': 1.2379403465730533e-24, 'stir': 1.2379403465730533e-24, 'mix': 1.2379403465730533e-24, 'locate': 1.2379403465730533e-24, 'boost': 1.2379403465730533e-24, 'Add': 1.2379403465730533e-24, 'hide': 1.2379403465730533e-24, 'arriv': 1.2379403465730533e-24, 'rescue': 1.2379403465730533e-24, 'carve': 1.2379403465730533e-24, 'remain': 1.2379403465730533e-24, 'spool': 1.2379403465730533e-24, 'fill': 1.2379403465730533e-24, 'look': 1.2379403465730533e-24, 'overcharge': 1.2379403465730533e-24, 'supply': 1.2379403465730533e-24, 'Drain': 1.2379403465730533e-24, 'crash': 1.2379403465730533e-24, 'bring': 1.2379403465730533e-24, 'beat': 1.2379403465730533e-24, 'meet': 1.2379403465730533e-24, 'hold': 1.2379403465730533e-24, 'show': 1.2379403465730533e-24, 'bank': 1.2379403465730533e-24, 'gather': 1.2379403465730533e-24, 'filter': 1.2379403465730533e-24, 'Take': 1.2379403465730533e-24, 'relieve': 1.2379403465730533e-24, 'concern': 1.2379403465730533e-24, 'change': 1.2379403465730533e-24, 'include': 1.2379403465730533e-24, 'bury': 1.2379403465730533e-24, 'spend': 1.2379403465730533e-24, 'transmit': 1.2379403465730533e-24, 'grant': 1.2379403465730533e-24, 'leak.': 1.2379403465730533e-24, 'extractant': 1.2379403465730533e-24, 'integrate': 1.2379403465730533e-24, 'introduce': 1.2379403465730533e-24, 'stash': 1.2379403465730533e-24, 'make': 1.2379403465730533e-24, 'identify': 1.2379403465730533e-24}, 'ED')\n",
      "({'be': 1.0, 'have': 1.1548224173015786e-17, 'contain': 3.221340285992516e-27, 'comprise': 1.603810890548638e-28, 'include': 2.9374821117108028e-30, 'make': 5.380186160021138e-32, 'hold': 2.4426007377405277e-36, 'consist': 2.4426007377405277e-36, 'compose': 2.4426007377405277e-36, 'use': 8.985825944049381e-37, 'show': 8.985825944049381e-37, 'attach': 3.3057006267607343e-37, 'place': 4.4737793061811207e-38, 'look': 1.6458114310822737e-38, 'connect': 1.6458114310822737e-38, 'provide': 1.6458114310822737e-38, 'move': 1.6458114310822737e-38, 'find': 1.6458114310822737e-38, 'develop': 6.054601895401186e-39, 'open': 6.054601895401186e-39, 'work': 6.054601895401186e-39, 'appear': 6.054601895401186e-39, 'br': 6.054601895401186e-39, 'i': 6.054601895401186e-39, 'feature': 6.054601895401186e-39, 'form': 6.054601895401186e-39, 'start': 6.054601895401186e-39, 'take': 6.054601895401186e-39, 'break': 2.2273635617957438e-39, 'click': 2.2273635617957438e-39, 'rotate': 2.2273635617957438e-39, 'set': 2.2273635617957438e-39, 'pres': 2.2273635617957438e-39, 'locate': 2.2273635617957438e-39, 'extend': 8.194012623990515e-40, 'design': 8.194012623990515e-40, 'construct': 8.194012623990515e-40, 'carve': 8.194012623990515e-40, 'display': 8.194012623990515e-40, 'combine': 8.194012623990515e-40, 'remove': 8.194012623990515e-40, 'fit': 8.194012623990515e-40, 'store': 8.194012623990515e-40, 'present': 8.194012623990515e-40, 'create': 8.194012623990515e-40, 'keep': 8.194012623990515e-40, 'mount': 8.194012623990515e-40, 'fill': 8.194012623990515e-40, 'see': 8.194012623990515e-40, 'enclose': 8.194012623990515e-40, 'divide': 3.0144087850653746e-40, 'result': 3.0144087850653746e-40, 'stand': 3.0144087850653746e-40, 'require': 3.0144087850653746e-40, 'rest': 3.0144087850653746e-40, 'decorate': 3.0144087850653746e-40, 'depend': 3.0144087850653746e-40, 'grow': 3.0144087850653746e-40, 'publish': 3.0144087850653746e-40, 'increase': 3.0144087850653746e-40, 'clean': 3.0144087850653746e-40, 'allow': 3.0144087850653746e-40, 'hang': 3.0144087850653746e-40, 'reflect': 3.0144087850653746e-40, 'accommodate': 3.0144087850653746e-40, 'run': 3.0144087850653746e-40, 'install': 3.0144087850653746e-40, 'measure': 3.0144087850653746e-40, 'push': 3.0144087850653746e-40, 'build': 3.0144087850653746e-40, 'perform': 3.0144087850653746e-40, 'involve': 3.0144087850653746e-40, 'reduce': 1.1089390193121365e-40, 'report': 1.1089390193121365e-40, 'reach': 1.1089390193121365e-40, 'regulate': 1.1089390193121365e-40, 'insert': 1.1089390193121365e-40, 'notice': 1.1089390193121365e-40, 'anchor': 1.1089390193121365e-40, 'lock': 1.1089390193121365e-40, 'wrap': 1.1089390193121365e-40, 'paint': 1.1089390193121365e-40, 'fix': 1.1089390193121365e-40, 'carry': 1.1089390193121365e-40, 'stretch': 1.1089390193121365e-40, 'come': 1.1089390193121365e-40, 'receive': 1.1089390193121365e-40, 'relate': 1.1089390193121365e-40, 'hit': 1.1089390193121365e-40, 'wear': 1.1089390193121365e-40, 'grab': 1.1089390193121365e-40, 'detach': 1.1089390193121365e-40, 'pull': 1.1089390193121365e-40, 'lift': 1.1089390193121365e-40, 'drag': 1.1089390193121365e-40, 'summarise': 1.1089390193121365e-40, 'touch': 1.1089390193121365e-40, 'support': 1.1089390193121365e-40, 'proces': 1.1089390193121365e-40, 'force': 1.1089390193121365e-40, 'draw': 1.1089390193121365e-40, 'feel': 1.1089390193121365e-40, 'lie': 1.1089390193121365e-40, 'operate': 1.1089390193121365e-40, 'transmit': 1.1089390193121365e-40, 'catch': 1.1089390193121365e-40, 'check': 1.1089390193121365e-40, 'walk': 1.1089390193121365e-40, 'launch': 1.1089390193121365e-40, 'compris': 1.1089390193121365e-40, 'judge': 1.1089390193121365e-40, 'pas': 1.1089390193121365e-40, 'correspond': 1.1089390193121365e-40, 'release': 1.1089390193121365e-40, 'give': 1.1089390193121365e-40, 'cause': 1.1089390193121365e-40, 'follow': 1.1089390193121365e-40, 'fall': 1.1089390193121365e-40, 'mark': 1.1089390193121365e-40, 'put': 1.1089390193121365e-40, 'power': 1.1089390193121365e-40, 'go': 1.1089390193121365e-40, 'eliminate': 4.0795586671775603e-41, 'pad': 4.0795586671775603e-41, 'stop': 4.0795586671775603e-41, 'leave': 4.0795586671775603e-41, 'Unwind': 4.0795586671775603e-41, 'fly': 4.0795586671775603e-41, 'span': 4.0795586671775603e-41, 'permit': 4.0795586671775603e-41, 'collapse': 4.0795586671775603e-41, 'recur': 4.0795586671775603e-41, 'get': 4.0795586671775603e-41, 'filter': 4.0795586671775603e-41, 'riffle': 4.0795586671775603e-41, 'control': 4.0795586671775603e-41, 'list': 4.0795586671775603e-41, 'zero': 4.0795586671775603e-41, 'engage': 4.0795586671775603e-41, 'account': 4.0795586671775603e-41, 'arrange': 4.0795586671775603e-41, 'unscrew': 4.0795586671775603e-41, 'shape': 4.0795586671775603e-41, 'probe': 4.0795586671775603e-41, 'choose': 4.0795586671775603e-41, 'prim': 4.0795586671775603e-41, 'shorten': 4.0795586671775603e-41, 'couple': 4.0795586671775603e-41, 'offer': 4.0795586671775603e-41, 'undertake': 4.0795586671775603e-41, 'snorgle': 4.0795586671775603e-41, 'flare': 4.0795586671775603e-41, 'lead': 4.0795586671775603e-41, 'unlock': 4.0795586671775603e-41, 'hatchet': 4.0795586671775603e-41, 'date': 4.0795586671775603e-41, 'scale': 4.0795586671775603e-41, 'curl': 4.0795586671775603e-41, 'strike': 4.0795586671775603e-41, 'cultivate': 4.0795586671775603e-41, 'dedicate': 4.0795586671775603e-41, 'survive': 4.0795586671775603e-41, 'load': 4.0795586671775603e-41, 'raise': 4.0795586671775603e-41, 'throw': 4.0795586671775603e-41, 'function': 4.0795586671775603e-41, 'play': 4.0795586671775603e-41, 'consider': 4.0795586671775603e-41, 'disregard': 4.0795586671775603e-41, 'vibrate': 4.0795586671775603e-41, 'expenditure': 4.0795586671775603e-41, 'guide': 4.0795586671775603e-41, 'signal': 4.0795586671775603e-41, 'cut': 4.0795586671775603e-41, 'Embroidered.': 4.0795586671775603e-41, 'pierc': 4.0795586671775603e-41, 'settle': 4.0795586671775603e-41, 'fee': 4.0795586671775603e-41, 'Recognized': 4.0795586671775603e-41, 'Point': 4.0795586671775603e-41, 'maximize': 4.0795586671775603e-41, 'pry': 4.0795586671775603e-41, 'circulate': 4.0795586671775603e-41, 'rig': 4.0795586671775603e-41, 'twist': 4.0795586671775603e-41, 'modify': 4.0795586671775603e-41, 'depict': 4.0795586671775603e-41, 'drive': 4.0795586671775603e-41, 'expos': 4.0795586671775603e-41, 'absorb': 4.0795586671775603e-41, 'shear': 4.0795586671775603e-41, 'hide': 4.0795586671775603e-41, 'collect': 4.0795586671775603e-41, 'roam': 4.0795586671775603e-41, 'scrape': 4.0795586671775603e-41, 'threaten': 4.0795586671775603e-41, 'position': 4.0795586671775603e-41, 'fire': 4.0795586671775603e-41, 'complement': 4.0795586671775603e-41, 'slice': 4.0795586671775603e-41, 'clasp': 4.0795586671775603e-41, 'become': 4.0795586671775603e-41, 'coupl': 4.0795586671775603e-41, 'observe': 4.0795586671775603e-41, 'stab': 4.0795586671775603e-41, 'pop': 4.0795586671775603e-41, 'stun': 4.0795586671775603e-41, 'protect': 4.0795586671775603e-41, 'bend': 4.0795586671775603e-41, 'curtain': 4.0795586671775603e-41, 'record': 4.0795586671775603e-41, 'grip': 4.0795586671775603e-41, 'redesign': 4.0795586671775603e-41, 'narrate': 4.0795586671775603e-41, 'help': 4.0795586671775603e-41, 'convince': 4.0795586671775603e-41, 'spin': 4.0795586671775603e-41, 'gleam': 4.0795586671775603e-41, 'malfunction': 4.0795586671775603e-41, 'swim': 4.0795586671775603e-41, 'lubricate': 4.0795586671775603e-41, 'change': 4.0795586671775603e-41, 'adjust': 4.0795586671775603e-41, 'slip': 4.0795586671775603e-41, 'root': 4.0795586671775603e-41, 'addres': 4.0795586671775603e-41, 'search': 4.0795586671775603e-41, 'remember': 4.0795586671775603e-41, 'scatter': 4.0795586671775603e-41, 'manufacture': 4.0795586671775603e-41, 'occur': 4.0795586671775603e-41, 'cleat': 4.0795586671775603e-41, 'thrust': 4.0795586671775603e-41, 'adopt': 4.0795586671775603e-41, 'grasp': 4.0795586671775603e-41, 'cover': 4.0795586671775603e-41, 'swish': 4.0795586671775603e-41, 'foresee': 4.0795586671775603e-41, 'calculate': 4.0795586671775603e-41, 'lower': 4.0795586671775603e-41, 'perfect': 4.0795586671775603e-41, 'refer': 4.0795586671775603e-41, 'begin': 4.0795586671775603e-41, 'heal': 4.0795586671775603e-41, 'paddle': 4.0795586671775603e-41, 'retract': 4.0795586671775603e-41, 'preside': 4.0795586671775603e-41, 'reveal': 4.0795586671775603e-41, 'post': 4.0795586671775603e-41, 'mean': 4.0795586671775603e-41, 'stick': 4.0795586671775603e-41, 'study': 4.0795586671775603e-41, 'graz': 4.0795586671775603e-41, 'kis': 4.0795586671775603e-41, 'opt': 4.0795586671775603e-41, 'cock': 4.0795586671775603e-41, 'beat': 4.0795586671775603e-41, 'contribute': 4.0795586671775603e-41, 'align': 4.0795586671775603e-41, 'flow': 4.0795586671775603e-41, 'flood': 4.0795586671775603e-41, 'purchase': 4.0795586671775603e-41, 'Assist': 4.0795586671775603e-41, 'insulate': 4.0795586671775603e-41, 'sense': 4.0795586671775603e-41, 'heat': 4.0795586671775603e-41, 'expand': 4.0795586671775603e-41, 'convert': 4.0795586671775603e-41, 'base': 4.0795586671775603e-41, 'float': 4.0795586671775603e-41, 'mix': 4.0795586671775603e-41, 'stay': 4.0795586671775603e-41, 'meet': 4.0795586671775603e-41, 'capture': 4.0795586671775603e-41, 'maintain': 4.0795586671775603e-41, 'save': 4.0795586671775603e-41, 'prepare': 4.0795586671775603e-41, 'withstand': 4.0795586671775603e-41, 'complain': 4.0795586671775603e-41, 'situate': 4.0795586671775603e-41, 'lose': 4.0795586671775603e-41, 'close': 4.0795586671775603e-41, 'improve': 4.0795586671775603e-41, 'wax': 4.0795586671775603e-41, 'eat': 4.0795586671775603e-41, 'detect': 4.0795586671775603e-41, 'clip': 4.0795586671775603e-41, 'belong': 4.0795586671775603e-41, 'fasten': 4.0795586671775603e-41, 'predict': 4.0795586671775603e-41, 'weaken': 4.0795586671775603e-41, 'scrub': 4.0795586671775603e-41, 'vary': 4.0795586671775603e-41, 'inspect': 4.0795586671775603e-41, 'publish.': 4.0795586671775603e-41, 'ascend': 4.0795586671775603e-41, 'scratch': 4.0795586671775603e-41, 'plant': 4.0795586671775603e-41, 'overhang': 4.0795586671775603e-41, 'average': 4.0795586671775603e-41, 'acces': 4.0795586671775603e-41, 'dangle': 4.0795586671775603e-41, 'sw': 4.0795586671775603e-41, 'lean': 4.0795586671775603e-41, 'tune': 4.0795586671775603e-41, 'thicken': 4.0795586671775603e-41, 'locate.': 4.0795586671775603e-41, 'prevent': 4.0795586671775603e-41, 'solder': 4.0795586671775603e-41, 'produce': 4.0795586671775603e-41, 'ha': 4.0795586671775603e-41, 'empty': 4.0795586671775603e-41, 'appreciate': 4.0795586671775603e-41, 'love': 4.0795586671775603e-41, 'procure': 4.0795586671775603e-41, 'hatch': 4.0795586671775603e-41, 'secrete': 4.0795586671775603e-41, 'block': 4.0795586671775603e-41, 'read': 4.0795586671775603e-41, 'behave': 4.0795586671775603e-41, 'explore': 4.0795586671775603e-41, 'erode': 4.0795586671775603e-41, 'gain': 4.0795586671775603e-41, 'recognize': 4.0795586671775603e-41, 'digest': 4.0795586671775603e-41, 'track': 4.0795586671775603e-41, 'collaborate': 4.0795586671775603e-41, 'analyze': 4.0795586671775603e-41, 'seek': 4.0795586671775603e-41, 'reinforce': 4.0795586671775603e-41, 'rise': 4.0795586671775603e-41, 'blink': 4.0795586671775603e-41, 'Combine': 4.0795586671775603e-41, 'step': 4.0795586671775603e-41, 'bounce': 4.0795586671775603e-41, 'crush': 4.0795586671775603e-41, 'delete': 4.0795586671775603e-41, 'outfit': 4.0795586671775603e-41, 'enter': 4.0795586671775603e-41, 'add': 4.0795586671775603e-41, 'consume': 4.0795586671775603e-41, 'share': 4.0795586671775603e-41, 'assist': 4.0795586671775603e-41, 'drape': 4.0795586671775603e-41}, 'CW')\n",
      "({'find': 0.8806532852884104, 'store': 0.11918346179776051, 'enclose': 0.00010868124940573345, 'discover': 3.998159729719537e-05, 'hide': 5.410920794468123e-06, 'keep': 5.410920794468123e-06, 'be': 1.99056651809187e-06, 'lock': 7.322884982902209e-07, 'hold': 7.322884982902209e-07, 'contain': 9.910447132702081e-08, 'include': 9.910447132702081e-08, 'carry': 3.6458497529375645e-08, 'wa': 3.6458497529375645e-08, 'put': 1.3412331697057122e-08, 'have': 4.934121089519397e-09, 'seal': 1.8151617090846239e-09, 'give': 1.8151617090846239e-09, 'make': 1.8151617090846239e-09, 'br': 6.677606751738515e-10, 'i': 6.677606751738515e-10, 'place': 6.677606751738515e-10, 'send': 6.677606751738515e-10, 'sit': 6.677606751738515e-10, 'contained.': 6.677606751738515e-10, 'show': 6.677606751738515e-10, 'full': 6.677606751738515e-10, 'see': 6.677606751738515e-10, 'eat': 2.456554240192215e-10, 'strike': 2.456554240192215e-10, 'serve': 2.456554240192215e-10, 'carried.': 2.456554240192215e-10, 'get': 2.456554240192215e-10, 'take': 2.456554240192215e-10, 'fit': 2.456554240192215e-10, 'in': 2.456554240192215e-10, 'pick': 2.456554240192215e-10, 'cover': 2.456554240192215e-10, 'open': 2.456554240192215e-10, 'come': 2.456554240192215e-10, 'tuck': 2.456554240192215e-10, 'use': 2.456554240192215e-10, 'cook': 2.456554240192215e-10, 'bring': 2.456554240192215e-10, 'goe': 9.037158010892493e-11, 'separate': 9.037158010892493e-11, 'preserve': 9.037158010892493e-11, 'coil': 9.037158010892493e-11, 'stor': 9.037158010892493e-11, 'scratch': 9.037158010892493e-11, 'secure': 9.037158010892493e-11, 'arrive': 9.037158010892493e-11, 'fold': 9.037158010892493e-11, 'intend': 9.037158010892493e-11, 'introduce': 9.037158010892493e-11, 'deliver': 9.037158010892493e-11, 'hang': 9.037158010892493e-11, 'receive': 9.037158010892493e-11, 'throw': 9.037158010892493e-11, 'insert': 9.037158010892493e-11, 'weigh': 9.037158010892493e-11, 'prefer': 9.037158010892493e-11, 'kill': 9.037158010892493e-11, 'bear': 9.037158010892493e-11, 'label': 9.037158010892493e-11, 'attach': 9.037158010892493e-11, 'return': 9.037158010892493e-11, 'inside': 9.037158010892493e-11, 'connect': 9.037158010892493e-11, 'Inside': 9.037158010892493e-11, 'found.': 9.037158010892493e-11, 'mis': 9.037158010892493e-11, 'cost': 9.037158010892493e-11, 'folder': 9.037158010892493e-11, 'print': 9.037158010892493e-11, 'pull': 9.037158010892493e-11, 'unpack': 9.037158010892493e-11, 'fill': 9.037158010892493e-11, 'stand': 9.037158010892493e-11, 'search': 9.037158010892493e-11, 'plac': 9.037158010892493e-11, 'accept': 9.037158010892493e-11, 'sup': 9.037158010892493e-11, 'fix': 9.037158010892493e-11, 'traverse': 9.037158010892493e-11, 'filled\\twith': 9.037158010892493e-11, 'stir': 9.037158010892493e-11, 'comprise': 9.037158010892493e-11, 'measure': 9.037158010892493e-11, 'dissolve': 9.037158010892493e-11, 'inventoried': 9.037158010892493e-11, 'press': 9.037158010892493e-11, 'house': 9.037158010892493e-11, 'Mak': 9.037158010892493e-11, 'examine': 9.037158010892493e-11, 'leave': 9.037158010892493e-11, 'tak': 9.037158010892493e-11, 'measur': 9.037158010892493e-11, 'explode': 9.037158010892493e-11, 'talk': 9.037158010892493e-11, 'pas': 9.037158010892493e-11, 'become': 9.037158010892493e-11, 'smuggle': 9.037158010892493e-11, 'sleep': 9.037158010892493e-11, 'look': 9.037158010892493e-11, 'snatch': 9.037158010892493e-11, 'arrange': 9.037158010892493e-11, 'remove': 9.037158010892493e-11, 'offer': 9.037158010892493e-11, 'cram': 9.037158010892493e-11, 'mark': 9.037158010892493e-11, 'face': 9.037158010892493e-11, 'ship': 9.037158010892493e-11, 'try': 9.037158010892493e-11, 'impart': 9.037158010892493e-11, 'alert': 9.037158010892493e-11, 'stored.': 9.037158010892493e-11, 'spill': 9.037158010892493e-11, 'protect': 9.037158010892493e-11, 'conceal': 9.037158010892493e-11, 'saw': 9.037158010892493e-11, 'fly': 9.037158010892493e-11, 'deposit': 9.037158010892493e-11, 'play': 9.037158010892493e-11, 'escape': 9.037158010892493e-11, 'lay.': 9.037158010892493e-11, 'buy': 9.037158010892493e-11, 'display': 9.037158010892493e-11, 'market': 9.037158010892493e-11, 'enter': 9.037158010892493e-11, 'seize': 9.037158010892493e-11, 'check': 9.037158010892493e-11, 'grow': 9.037158010892493e-11, 'mistake': 9.037158010892493e-11, 'notice': 9.037158010892493e-11}, 'CC')\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    # Compute softmax values for each sets of scores in x.\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_dict(d: dict) -> dict:\n",
    "    x = []\n",
    "    for entry in d.items():\n",
    "        x.append(entry[1])\n",
    "    for softmaxed_value, key in zip(softmax(x), d):\n",
    "        d[key] = softmaxed_value\n",
    "    return d\n",
    "\n",
    "SOFTMAXED_DICTS = []\n",
    "for d in DICTS:\n",
    "    SOFTMAXED_DICTS.append((softmax_dict(d[0]), d[1]))\n",
    "\n",
    "for dic in SOFTMAXED_DICTS:\n",
    "    print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT maskierten satz geben\n",
    "def bert_predict(sentence):\n",
    "    model = pipeline('fill-mask', model='bert-base-uncased')\n",
    "    pred = model(sentence)\n",
    "    # bert wahrscheinlichkeiten neu verteilen\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# verb stemmen\n",
    "def get_verb_lemma(verb: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(verb, pos='v')\n",
    "\n",
    "# mit dicts vergleichen\n",
    "def predict_class(bert_predictions, class_dicts):\n",
    "    scores = []\n",
    "    score = 0\n",
    "    for d in class_dicts:\n",
    "        for prediction in bert_predictions:\n",
    "            try:\n",
    "                verb = get_verb_lemma(prediction['token_str'])\n",
    "                score += d[0][verb] * prediction['score']\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        scores.append((score))\n",
    "        score = 0\n",
    "    \n",
    "    # softmax and sort scores and add labels\n",
    "    softmaxed_scores = softmax(np.array(scores))\n",
    "    labeled_scores = []\n",
    "    for score, label in zip(softmaxed_scores, class_dicts):\n",
    "        labeled_scores.append([label[1], score])\n",
    "    labeled_scores = sorted(labeled_scores, key=lambda item: item[1], reverse=True)\n",
    "    return labeled_scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction test!\n",
    "This cell tests the prediction algorithm used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.6600832939147949, 'token': 3594, 'token_str': 'uses', 'sequence': 'the author of a keygen uses a disassembler to look at the raw assembly code.'}, {'score': 0.0492115244269371, 'token': 3791, 'token_str': 'needs', 'sequence': 'the author of a keygen needs a disassembler to look at the raw assembly code.'}, {'score': 0.03457473963499069, 'token': 5942, 'token_str': 'requires', 'sequence': 'the author of a keygen requires a disassembler to look at the raw assembly code.'}, {'score': 0.031678952276706696, 'token': 9005, 'token_str': 'creates', 'sequence': 'the author of a keygen creates a disassembler to look at the raw assembly code.'}, {'score': 0.020806970074772835, 'token': 3640, 'token_str': 'provides', 'sequence': 'the author of a keygen provides a disassembler to look at the raw assembly code.'}]\n",
      "[['IA', 0.2533222748040858], ['CW', 0.1467432012481933], ['PP', 0.11099706230550001], ['CT', 0.08839361869568539], ['MC', 0.08786752310573927], ['EO', 0.08535595203009476], ['CC', 0.08535595203009476], ['CE', 0.07199030606444565], ['ED', 0.06997410971616105]]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The author of a keygen [MASK] a disassembler to look at the raw assembly code.'\n",
    "pred = bert_predict(sentence)\n",
    "scores = predict_class(pred, LOG_DICTS)\n",
    "print(pred)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class name decoding cell\n",
    "Converts class names to two digit values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_short_class(long_class: str) -> str:\n",
    "    if long_class == 'Component-Whole(e2,e1)' or long_class == 'Component-Whole(e1,e2)':\n",
    "        return 'CW'\n",
    "    if long_class == 'Instrument-Agency(e2,e1)' or long_class == 'Instrument-Agency(e1  ,e2)':\n",
    "        return 'IA'\n",
    "    if long_class == 'Member-Collection(e1,e2)' or long_class == 'Member-Collection(e2,e1)':\n",
    "        return 'MC'\n",
    "    if long_class == 'Cause-Effect(e2,e1)' or long_class == 'Cause-Effect(e1,e2)':\n",
    "        return 'CE'\n",
    "    if long_class == 'Entity-Destination(e2,e1)' or long_class == 'Entity-Destination(e1,e2)':\n",
    "        return 'ED'\n",
    "    if long_class == 'Content-Container(e2,e1)' or long_class == 'Content-Container(e1,e2)':\n",
    "        return 'CC'\n",
    "    if long_class == 'Message-Topic(e2,e1)' or long_class == 'Message-Topic(e1,e2)':\n",
    "        return 'MT'\n",
    "    if long_class == 'Product-Producer(e2,e1)' or long_class == 'Product-Producer(e1,e2)':\n",
    "        return 'PP'\n",
    "    if long_class == 'Entity-Origin(e2,e1)' or long_class == 'Entity-Origin(e1,e2)':\n",
    "        return 'EO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(sentence_number) \u001b[38;5;241m<\u001b[39m START_AT_SENTENCE_INDEX:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mbert_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m scores \u001b[38;5;241m=\u001b[39m predict_class(pred, LOG_DICTS)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero_shot_results_LOG_DICTS.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m, in \u001b[0;36mbert_predict\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbert_predict\u001b[39m(sentence):\n\u001b[1;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfill-mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(sentence)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# bert wahrscheinlichkeiten neu verteilen\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:905\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 905\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    915\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    916\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:279\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     )\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    281\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:2894\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2892\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild_in_name_scope()  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[0;32m   2893\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2894\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_in_name_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# build the network with dummy inputs\u001b[39;00m\n\u001b[0;32m   2896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safetensors_from_pt:\n\u001b[0;32m   2897\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1129\u001b[0m, in \u001b[0;36mTFPreTrainedModel.build_in_name_scope\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_in_name_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1478\u001b[0m, in \u001b[0;36mTFBertForMaskedLM.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m-> 1478\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlm\u001b[38;5;241m.\u001b[39mname):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:1035\u001b[0m, in \u001b[0;36mTFBertMainLayer.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler\u001b[38;5;241m.\u001b[39mname):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:678\u001b[0m, in \u001b[0;36mTFBertEncoder.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(layer\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m--> 678\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:591\u001b[0m, in \u001b[0;36mTFBertLayer.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m--> 591\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    593\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate\u001b[38;5;241m.\u001b[39mname):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:438\u001b[0m, in \u001b[0;36mTFBertAttention.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense_output\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_output\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m--> 438\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:384\u001b[0m, in \u001b[0;36mTFBertSelfOutput.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense\u001b[38;5;241m.\u001b[39mname):\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayerNorm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm\u001b[38;5;241m.\u001b[39mname):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tf_keras\\src\\layers\\core\\dense.py:164\u001b[0m, in \u001b[0;36mDense.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m     shape\u001b[38;5;241m=\u001b[39m[last_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py:699\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout:\n\u001b[0;32m    697\u001b[0m     getter \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(getter, layout\u001b[38;5;241m=\u001b[39mlayout)\n\u001b[1;32m--> 699\u001b[0m variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_variable_with_custom_getter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(allenl): a `make_variable` equivalent should be added as a\u001b[39;49;00m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# `Trackable` method.\u001b[39;49;00m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgetter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgetter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Manage errors in Layer rather than Trackable.\u001b[39;49;00m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m regularizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;66;03m# TODO(fchollet): in the future, this should be handled at the\u001b[39;00m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;66;03m# level of variable creation, and weight regularization losses\u001b[39;00m\n\u001b[0;32m    720\u001b[0m     \u001b[38;5;66;03m# should be variable attributes.\u001b[39;00m\n\u001b[0;32m    721\u001b[0m     name_in_scope \u001b[38;5;241m=\u001b[39m variable\u001b[38;5;241m.\u001b[39mname[: variable\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\trackable\\base.py:492\u001b[0m, in \u001b[0;36mTrackable._add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    482\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (checkpoint_initializer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    483\u001b[0m       \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(initializer, CheckpointInitialValueCallable) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    484\u001b[0m            (initializer\u001b[38;5;241m.\u001b[39mrestore_uid \u001b[38;5;241m>\u001b[39m checkpoint_initializer\u001b[38;5;241m.\u001b[39mrestore_uid))):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;66;03m# then we'll catch that when we call _track_trackable. So this is\u001b[39;00m\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;66;03m# \"best effort\" to set the initializer with the highest restore UID.\u001b[39;00m\n\u001b[0;32m    491\u001b[0m     initializer \u001b[38;5;241m=\u001b[39m checkpoint_initializer\n\u001b[1;32m--> 492\u001b[0m new_variable \u001b[38;5;241m=\u001b[39m \u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_for_getter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# If we set an initializer and the variable processed it, tracking will not\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# assign again. It will add this variable to our dependencies, and if there\u001b[39;00m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;66;03m# is a non-trivial restoration queued, it will handle that. This also\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# handles slot variables.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_variable, Trackable):\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:137\u001b[0m, in \u001b[0;36mmake_variable\u001b[1;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner, layout, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m    130\u001b[0m     use_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# In theory, in `use_resource` is True and `collections` is empty\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# (that is to say, in TF2), we can use tf.Variable.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# However, this breaks legacy (Estimator) checkpoints because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# it changes variable names. Remove this when V1 is fully deprecated.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvariable_shape\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# noqa: E501\u001b[39;49;00m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtensor\u001b[38;5;241m.\u001b[39mDVariable(\n\u001b[0;32m    154\u001b[0m         initial_value\u001b[38;5;241m=\u001b[39minit_val,\n\u001b[0;32m    155\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m         shape\u001b[38;5;241m=\u001b[39mvariable_shape \u001b[38;5;28;01mif\u001b[39;00m variable_shape \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:197\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    196\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_variable_call\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_call):\n\u001b[1;32m--> 197\u001b[0m     variable_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m variable_call\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_v1.py:307\u001b[0m, in \u001b[0;36mVariableV1._variable_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape, experimental_enable_variable_lifting, expected_shape, collections, use_resource, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggregation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m   aggregation \u001b[38;5;241m=\u001b[39m variables\u001b[38;5;241m.\u001b[39mVariableAggregation\u001b[38;5;241m.\u001b[39mNONE\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprevious_getter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimport_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpected_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_resource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_v1.py:300\u001b[0m, in \u001b[0;36mVariableV1._variable_call.<locals>.<lambda>\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m VariableV1:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m previous_getter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mdefault_variable_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, getter \u001b[38;5;129;01min\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\u001b[38;5;241m.\u001b[39m_variable_creator_stack:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    302\u001b[0m   previous_getter \u001b[38;5;241m=\u001b[39m variables\u001b[38;5;241m.\u001b[39m_make_getter(getter, previous_getter)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\variable_v1.py:46\u001b[0m, in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_variable_creator\u001b[39m(next_creator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ref_variable  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mref_variable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_variable_creator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnext_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnext_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\ref_variable.py:67\u001b[0m, in \u001b[0;36mdefault_variable_creator\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_resource:\n\u001b[0;32m     66\u001b[0m   distribute_strategy \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribute_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResourceVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvariable_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m      \u001b[49m\u001b[43mimport_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimport_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m RefVariable(\n\u001b[0;32m     84\u001b[0m       initial_value\u001b[38;5;241m=\u001b[39minitial_value,\n\u001b[0;32m     85\u001b[0m       trainable\u001b[38;5;241m=\u001b[39mtrainable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m       aggregation\u001b[38;5;241m=\u001b[39maggregation,\n\u001b[0;32m     97\u001b[0m       shape\u001b[38;5;241m=\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\variables.py:200\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m variable_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m variable_call\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mVariableMetaclass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1829\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape, handle, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m   1824\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_from_handle(trainable\u001b[38;5;241m=\u001b[39mtrainable,\n\u001b[0;32m   1825\u001b[0m                          shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   1826\u001b[0m                          dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1827\u001b[0m                          handle\u001b[38;5;241m=\u001b[39mhandle)\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1829\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_from_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1830\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1831\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaching_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1835\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m      \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1838\u001b[0m \u001b[43m      \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1840\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1841\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperimental_enable_variable_lifting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:2028\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape, experimental_enable_variable_lifting)\u001b[0m\n\u001b[0;32m   2026\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2027\u001b[0m   shape \u001b[38;5;241m=\u001b[39m initial_value\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m-> 2028\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43meager_safe_variable_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_in_graph_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2034\u001b[0m handle\u001b[38;5;241m.\u001b[39m_parent_trackable \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   2035\u001b[0m handle\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m handle_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:241\u001b[0m, in \u001b[0;36meager_safe_variable_handle\u001b[1;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a variable handle with information to do shape inference.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03mThe dtype is read from `initial_value` and stored in the returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m  The handle, a `Tensor` of type `resource`.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m dtype \u001b[38;5;241m=\u001b[39m initial_value\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_variable_handle_from_shape_and_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mgraph_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:169\u001b[0m, in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[1;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInternalError(\n\u001b[0;32m    163\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    164\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing an explicit shared_name is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed when executing eagerly.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m   shared_name \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39manonymous_name()\n\u001b[1;32m--> 169\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_handle_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m   initial_value \u001b[38;5;241m=\u001b[39m handle\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1276\u001b[0m, in \u001b[0;36mvar_handle_op\u001b[1;34m(dtype, shape, container, shared_name, debug_name, allowed_devices, name)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   1275\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1276\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVarHandleOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshared_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshared_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallowed_devices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_devices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   1281\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('test_sentences.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# set to start at sentence x of file, useful if script crashed midway\n",
    "START_AT_SENTENCE_INDEX = 3299\n",
    "\n",
    "for index, line in enumerate(lines):\n",
    "    \n",
    "    # zeilen mit maskiertem satz\n",
    "    if index % 5 == 1:\n",
    "        # we are not predicting here because we do not want to waste time on predicting sentences with class 'other'\n",
    "        sentence = line\n",
    "        sentence_number = line.split('\\t')[0]\n",
    "\n",
    "    # zeile mit label\n",
    "    if index % 5 == 2:\n",
    "        y = get_short_class(line.strip())\n",
    "\n",
    "    # predict (bei kommentarzeile)\n",
    "    if index % 5 == 3:\n",
    "        # skip class: Other\n",
    "        if y == None:\n",
    "            continue\n",
    "        # skip sentence if no mask token found\n",
    "        if not re.search(r\"\\[MASK\\]\", sentence):\n",
    "            continue\n",
    "        # skip until wanted sentence\n",
    "        if int(sentence_number) < START_AT_SENTENCE_INDEX:\n",
    "            continue\n",
    "        pred = bert_predict(sentence)\n",
    "        scores = predict_class(pred, LOG_DICTS)\n",
    "\n",
    "        with open('zero_shot_results_LOG_DICTS.csv', 'a') as results:\n",
    "            results.write(f\"{sentence_number},{y},{scores[0][0]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 6431\n",
      "Davon korrekt: 2443\n",
      "0.37987871248639404\n"
     ]
    }
   ],
   "source": [
    "with open('zero_shot_results_LOG.csv', 'r') as csv:\n",
    "    lines = csv.readlines()\n",
    "\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    correct_count = 0\n",
    "    for index, line in enumerate(lines):\n",
    "\n",
    "\n",
    "        line = line.split(',')\n",
    "        count += 1\n",
    "        line[2] = line[2][:2]\n",
    "        if line[1] == line[2]:\n",
    "            correct_count += 1\n",
    "\n",
    "\n",
    "print(f\"Count: {count}\")\n",
    "print(f\"Davon korrekt: {correct_count}\")\n",
    "print(correct_count / count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_predict_class import bert_predict\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import softmax_and_log_dicts\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import softmax\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def get_cosine_sim(A: list[float], B: list[float]) -> float:\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "# takes in a sentence and returns an array with sentences containing each one [UNK] token from left to right.\n",
    "# [MASK] won't be mapped to [UNK]\n",
    "def get_sub_sentences(sentence: str) -> list[str]:\n",
    "    sub_sentences = list()\n",
    "    tokens = sentence[:-1].split(' ')\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        s = \"\"\n",
    "        for j in range(len(tokens)):\n",
    "            if i == j:\n",
    "                if tokens[j] == '[MASK]':\n",
    "                    s += '[MASK] '\n",
    "                else:\n",
    "                    s += '[UNK] '\n",
    "            else:\n",
    "                s += tokens[j] + ' '\n",
    "\n",
    "        sub_sentences.append(s[:-1] + '.')\n",
    "\n",
    "    return sub_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# verb stemmen\n",
    "def get_verb_lemma(verb: str) -> str:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(verb, pos='v')\n",
    "\n",
    "# mit dicts vergleichen\n",
    "def predict_class(bert_predictions, class_dicts=softmax_and_log_dicts.LOG_DICTS):\n",
    "    scores = []\n",
    "    score = 0\n",
    "    for d in class_dicts:\n",
    "        for prediction in bert_predictions:\n",
    "            try:\n",
    "                verb = get_verb_lemma(prediction['token_str'])\n",
    "                score += d[0][verb] * prediction['score']\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        scores.append((score))\n",
    "        score = 0\n",
    "    \n",
    "    # softmax and sort scores and add labels\n",
    "    softmaxed_scores = softmax.softmax(np.array(scores))\n",
    "    return softmaxed_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: get tagged words marked in output by providing original sentence with XML tags\n",
    "orig_sentence = \"Their <e1>composer</e1> has sunk into <e2>oblivion</e2>.\"\n",
    "xml_indices = []\n",
    "for index, token in enumerate(orig_sentence.split(' ')):\n",
    "    if re.search('<e[12]>.*</e[12]>', token):\n",
    "        xml_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Their composer has [MASK] into oblivion.\"\n",
    "\n",
    "# first get the prediction from the original sentence so we have something to compare our next results with\n",
    "bert_pred = bert_predict(sentence)\n",
    "class_pred = predict_class(bert_pred)\n",
    "\n",
    "class_predictions = []\n",
    "for sub_sentence in get_sub_sentences(sentence):\n",
    "    bert_sub_pred = bert_predict(sub_sentence)\n",
    "    class_sub_pred = predict_class(bert_sub_pred)\n",
    "    class_predictions.append(class_sub_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "for p in class_predictions:\n",
    "    cosine_similarities.append(get_cosine_sim(p, class_pred))\n",
    "\n",
    "\n",
    "# Calculate min and max\n",
    "X_min = min(cosine_similarities)\n",
    "X_max = max(cosine_similarities)\n",
    "\n",
    "# Scale the array\n",
    "scaled_similarities = [(x - X_min) / (X_max - X_min) for x in cosine_similarities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9972942570641264,\n",
       " 0.9948524434381655,\n",
       " 0.7415548887139448,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.7219479625770411]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-direction: row; font-size: xx-large; '><div style='color: rgb(255, 254.31553920016523, 254.31553920016523)'>The&nbsp</div><div style='color: rgb(255, 13.047286473982696, 13.047286473982696)'>burst&nbsp</div><div style='color: rgb(255, 167.91837631151947, 167.91837631151947)'>has&nbsp</div><div style='color: rgb(255, 204.39599525725706, 204.39599525725706)'>been&nbsp</div><div style='color: rgb(255, 255.0, 255.0)'>[MASK]&nbsp</div><div style='color: rgb(255, 0.0, 0.0)'>by&nbsp</div><div style='color: rgb(255, 142.50769038096183, 142.50769038096183)'>water&nbsp</div><div style='color: rgb(255, 232.81884445953, 232.81884445953)'>hammer&nbsp</div><div style='color: rgb(255, 126.65993168306237, 126.65993168306237)'>pressure</div>.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "STR = \"<div style='display: flex; flex-direction: row; font-size: xx-large; '>\"\n",
    "for index, token in enumerate(sentence[:-1].split(' ')):\n",
    "    STR += f\"<div style='color: rgb(255, {scaled_similarities[index] * 255}, {scaled_similarities[index] * 255})'>{token}&nbsp</div>\"\n",
    "STR = STR[:-11] + \"</div>.</div>\"\n",
    "\n",
    "HTML(STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "function js_in_ipynb_lul(){\n",
       "    alert(\"brrrrrrrrrrt fuer die welt!\")\n",
       "}\n",
       "</script>\n",
       "<button onclick=\"js_in_ipynb_lul()\">click me</button>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_str = \"\"\"\n",
    "<script>\n",
    "function js_in_ipynb_lul(){\n",
    "    alert(\"brrrrrrrrrrt fuer die welt!\")\n",
    "}\n",
    "</script>\n",
    "<button onclick=\"js_in_ipynb_lul()\">click me</button>\n",
    "\"\"\"\n",
    "\n",
    "HTML(html_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
